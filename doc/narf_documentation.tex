\documentclass[letterpaper]{report}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{float}
\usepackage{subfigure}
 
\newenvironment{qanda}{\setlength{\parindent}{0pt}}{\bigskip}
\newcommand{\Q}{\bigskip\bfseries Q: }
\newcommand{\A}{\par\textbf{A:} \normalfont}
\newcommand{\definition}[1]{\textbf{#1}}
\newcommand{\matlab}[1]{\texttt{#1}}

\setcounter{secnumdepth}{2}

\title{Neural Activity Regression Framework (NARF) Documentation}
\author{Ivar Thorson}
  
\begin{document}


\maketitle
\tableofcontents

\chapter{Basic Functionality and Data Structures}

This document was written to clarify and document design decisions made regarding the Neural Activity Regression Framework (NARF) implemented in the Brain, Hearing and Behavior Laboratory of the Oregon Health and Science University (OHSU), under the supervision of Prof. Stephen David. The intended audience are end users who want to use NARF to analyze their neural data, and also for those intrepid programmers who want to implement their own models in the NARF framework. 

\section{NARF in a Paragraph} 

NARF is a MATLAB framework that was written to let us create and fit the parameters of thousands of different mathematical models of auditory neural activity. It is mathematically very general and does not assume a specific model structure -- neural or otherwise -- and could presumably be used to model many types of physical systems. It comes with a large collection of input-output building blocks, or ``modules'', from which more complex models may be built. Users are encouraged to write their own modules to model specific aspects of a system, although a variety of default modules are already available, including various nonlinear functions, convolution kernels, sample-rate conversions, filterbanks, differential equations, and file-loading modules. NARF also includes algorithms for fitting some or all of the parameters of modules, including user-defined modules, so that you may easily find the parameters that best describe your data. Fitting algorithms may be fairly sophisticated and are also user-definable. It is not unusual for us to fit some model parameters on one set of data, some parameters on another set of data, and verifying the entire model on a third subset of the data. If the model may be represented as a directed graph, when parameters are changed during fitting, only the minimum necessary will be recomputed at any time. After a model has been fit to the data, relevant statistics (also user-definable) may be logged and stored in a database. A GUI browser lets users quickly compare fits among different cells or models, group cells by experiment, record notes about each experiment, and generate a variety of useful plots. It is often necessary to test a model across a population of neurons to be sure that the model is not overly specific to a particular neural data set, so NARF has significant support for plotting the performance of a model across a population, querying the database, and queuing up collections of models to be fit for every neuron in a data set. Although not strictly a part of NARF, it integrates with the BAPHY and CellDB systems for the management of large amounts neural data. To date NARF has fit more than 700,000 models to neural data sets, requiring approximately 10 computer-years of time, if we consider the fact that we have run a computer cluster with seven nodes constantly for the last two years.

\section{Problem and Approach}

\subsection{The Problem NARF Attempts to Solve}

There are many possible mathematical models which could be used describe the spiking activity of a neuron that is given some sensory stimulus. In our case, we will be considering mostly models of neurons in the auditory cortex of a mammal, usually a ferret, and usually near the A1 region. Neurons in this region of the brain often respond preferentially to a particular auditory frequency. For example, some neurons may spike continously whenever any sound has accoustic energy near 1.2KHz. Another neuron may spike frequently only on the onset of a 5KHz tone hidden in a ferret vocalization played to the  ferret's left ear while it is thinking about dinner. Our goal is to play recordings of various sounds, measure neural activity, and attempt to discover as much as we can about the neuron(s) we are studying.

Mathematically, the problem is to develop an equation and associated parameters that can accurately predict how a neuron will fire when a new, as-yet-unheard sound is played. This is a classic inference problem, in which we seek to infer which system produced the neural response that we measured. If the model's prediction closely matches the measured response of the neuron, then we have learned something significant about the neuron. The better our ability to predict the neuron's activity, the better our understanding of it, and the more we may learn through experiment about how neural activity is affected by various factors. Our ability to learn about the dynamics of neurons through experiment is thus closely related to our ability to model each neuron.

Fortunately or unfortunately, while the neuroscientist community has developed a plethora of mathematical models which can describe this behavior, there remain few guidelines on which model should be used or how model parameters should be found. The difficulty for most scientists is in the implementation of existing models; it is often a significant amount of work to adapt another person's model to your needs, and significantly more work to develop one yourself. It's hard to compare your work to other scientists'. Ideally, we would be able to easily fitting several different models to the measured activity of the neuron, so that we might quickly identify the characteristics of the neuron under study, as well as give us the ability to compare our work with other researchers'. It would also be advantageous to be able to recombine parts of the model which were particularly useful in describing neural activity such that variations on successfully predictive models could be quickly examined. 

\subsection{NARF's Approach: Pure Function Composition}

Most modeling approaches are highly domain-specific; modeling the a biological system often requires strong assumptions about the nature of the signals being processed and correlated. What works well for modeling the auditory cortex may not work well for modeling activity in the motor cortex. About the only thing we can be sure of is that we will be using mathematical functions, so we begin there. If we consider each aspect of a mathematical model to be a \definition{pure function} $f_1$ which accepts an argument $x_1$ and returns a value $x_2$, we can write it as

\begin{equation}
  f_1(x_1) \rightarrow x_2
\end{equation}

The notion of a 'pure' function is simply that it will always produce the same result when given the same input. This is familiar to most mathematicians, but the idea of a function is usually corrupted in the domain of computer science to mean just a ``subroutine'' of instructions. To be more specific, a pure function is on in which there are no side effects (i.e. no print statements, no changing of global variables, no writing to disk, and no use of global variables). A pure function is a complete black box whose outputs only depend on its explicit arguments and inputs. If a function's computation is based in any way on a global variable, then it is possible for the same function, given the same arguments, to produce different results depending on the value of the global variable at the time it was run. This should be avoided to maintain mathematical elegance. Most modules in NARF have their main function (usually prefixed with ``do\_'' by convention) that maps a set of inputs to outputs, and this function should always be pure. Indeed, it is often an explicit goal of a seasoned functional programmer to keep the number of non-pure functions to a minumum; but we digress and must return to the operation of NARF. 

If we make a neural model out of more than one pure function (e.g. $f_1, f_2, ..., f_n$), then we could potentially chain the output of one function to the input of the next:

\begin{eqnarray*}
f_{1}(x_{1}) & \rightarrow & x_{2}\\
f_{2}(x_{2}) & \rightarrow & x_{3}\\
 & \vdots\\
f_{n}(x_{n}) & \rightarrow & x_{n+1}
\end{eqnarray*}

And we will always get the same output ($x_{n+1}$) at the end of the computational chain. In a perhaps more familiar notation we could have written

\begin{equation}
x_{4} = f_3(f_2(f_1(x_1)))
\end{equation}

All of the above has assumed that every function has no parameters. If we had a function which added 23 to it's input argument and then divided by three ($f_1(x_1) = (x_1 + 23) / 3 $), then we would be required to define a completely new function $f_1$ if we wanted to try adding 25 or dividing by a different number. If some part of the function's behavior will be changing regularly and many values must be tried, then it makes sense to bundle these changing parts into a model parameter vector $\phi_1 = [\phi_{11} \phi_{12}] = 23 3$ and define our function as $f_1(x_1, \phi_1) = (x_1 + \phi_{11}) / \phi_{12}$.

Therefore, NARF assumes that every mathematical function has two arguments

\begin{equation}
  f_1(x_1, \phi_1) \rightarrow x_2
\end{equation}

and that they can be composed together to create chains of pure functions. 

\begin{eqnarray*}
f_{1}(x_{1}, \phi_1) & \rightarrow & x_{2}\\
f_{2}(x_{2}, \phi_2) & \rightarrow & x_{3}\\
 & \vdots\\
f_{n}(x_{n}, \phi_n) & \rightarrow & x_{n+1}
\end{eqnarray*}

Now we can describe nearly any neural model by specifying some initial conditions $x_1$, a list of functions $f_1, f_2, ..., f_n$, and a list of parameter vectors $\phi_1, \phi_2, ..., \phi_n$. By executing the functions one after another, our prediction about the neural activity will finally be held in $x_{n+1}$. If we want, we can observe intermediate values of $x$ to better understand how the model is functioning as a whole. Also, this formalism clearly separates the things we want to find at the end of the day (the parameters $\phi$) from the values that we need to compute to get work done ($x$). 

\section{The Most Important Global Variables: STACK and XXX}

MATLAB's data types prohibit us from being completely general with regard to how we represent our functions $f$ and our data $x$. Data structures are never perfectly general, but some are more flexible than others, and for research its generally wiser to choose flexibility over raw speed. The approach used in NARF was to use MATLAB's cell arrays as much as possible because of their generality and loose ordering of data along different dimensions. 

The structure which holds the list of functions $f_1, f_2, ...$ and parameters $\phi$ is a global variable called \matlab{STACK} because it represents a big fat stack of transformations on the input data. The structure which holds all the $x_1, x_2, ..., x_{n+1}$ is another global variable called \matlab{XXX} due to the fact that each $x$ is dependent upon the previous and so it resembles a chain of x's. Also, XXX happens to be very visible but concise pattern, which is nice when you are staring at a huge block of matlab code. Each cell of \matlab{STACK} and \matlab{XXX} is a structure containing named fields. You may put any fields you like in each cell, except for the fields defined in the next two sections, which have special meaning to NARF and are used for special purposes. 

\subsection{What are Modules and Signals?}

\matlab{STACK} was described as a chain of modules, but we should first define what exactly a module is. Because so often it is convenient to keep a function $f$ and its associated parameters $\phi$ together, we will call them a \definition{module}, and use each module as an input-output block that models some small piece of functionality. Each module has subfields that we are calling \definition{parameters} because they represent named values of $\phi$. We'll see some examples in a few sections. 

The data values $x_1, x_2, ...$ of \matlab{XXX} are also represented as structs. We name the fields of each struct a \definition{signals}, because typically they are long vectors of neural or electrical signals that were sampled at a regular rate. Signals are a way of naming your variables going in and out of a module in a consistent way. We chose to organize our signals in a convenient way for our data; however you may find organizing your signals in a different way is more convenient. The choice is left up to you, and you may name your signals as you choose. 

\subsection{Module Discovery and the \matlab{MODULES} Global Data Structure}

Whenever a NARF GUI starts running, it searchers for modules that in the \matlab{modules/} directory upon startup and attempts to load them into a structure called \matlab{MODULES}. MATLAB is pretty good about allowing you to edit modules dynamically, but if you are adding new modules after starting a GUI, you may need to call the function \matlab{scan\_directory\_for\_modules.m} with the appropriate to re-scan them. 

Once discovered, the global data structure \matlab{MODULES} is populated so that under each field is one of the objects returned by the modules. You could manually create such a global data structure like this:

\begin{verbatim}
>> global MODULES;
>> MODULES = scan_directory_for_modules()
MODULES = scan_directory_for_modules()
Scanning dir for modules: /home/ivar/matlab/narf/modules
Found 'add_nth_order_terms.m'
Found 'bandpass_filter_bank.m'
Found 'bayesian_likelihood.m'
Found 'bernoulli_trials.m'
Found 'concatenate_channels.m'
Found 'correlation.m'
Found 'depression_filter_bank.m'
Found 'depression_filter_bank_nonorm.m'
Found 'downsample_signal.m'
Found 'error_norm.m'
Found 'fir_filter.m'
Found 'fir_separable_filter.m'
Found 'fourier_transform.m'
Found 'gammatone_filter.m'
Found 'gammatone_filter_bank.m'
Found 'gmm_nonlinearity.m'
Found 'hilbert_transform.m'
Found 'infer_respavg.m'
Found 'int_fire_neuron.m'
Found 'inter_spike_intervals.m'
Found 'likelihood_poisson.m'
Found 'lindeberg_filter.m'
Found 'lindeberg_spectral.m'
Found 'load_SNS_from_baphy.m'
Found 'load_stim_resps_from_baphy.m'
Found 'load_stim_resps_wehr.m'
Found 'mean_squared_error.m'
Found 'mutual_information.m'
Found 'neural_statistics.m'
Found 'nim_wrapper.m'
Found 'nonlinearity.m'
Found 'nonparm_filter_nonlinearity.m'
Found 'nonparm_gain.m'
Found 'nonparm_nonlinearity.m'
Found 'nonparm_nonlinearity_2d.m'
Found 'normalize_channels.m'
Found 'passthru.m'
Found 'pole_zeros.m'
Skipping pole_zeros_quadratic.m due to error: Error: File: pole_zeros_quadratic.m Line: 175 Column: 10
Function definition is misplaced or improperly nested.
Found 'pz_synapse.m'
Found 'pz_wavelet.m'
Found 'pz_wavelet_filterbank.m'
Found 'reparameterize.m'
Skipping response_metrics.m due to error: Error: File: response_metrics.m Line: 53 Column: 19
Expression or statement is incomplete or incorrect.
Found 'smooth_respavg.m'
Found 'sparse_empirical_nonlinearity.m'
Found 'spike_times.m'
Found 'spike_triggered_average.m'
Found 'split_signal.m'
Found 'state_space_diffeq.m'
Found 'subsample_channels.m'
Found 'sum_fields.m'
Found 'sum_vector_elements.m'
Found 'truncate_data.m'
Found 'unify_signal.m'
Found 'weight_channels.m'

MODULES = 

              add_nth_order_terms: [1x1 struct]
             bandpass_filter_bank: [1x1 struct]
              bayesian_likelihood: [1x1 struct]
                 bernoulli_trials: [1x1 struct]
             concatenate_channels: [1x1 struct]
                      correlation: [1x1 struct]
           depression_filter_bank: [1x1 struct]
    depression_filter_bank_nonorm: [1x1 struct]
                downsample_signal: [1x1 struct]
                       error_norm: [1x1 struct]
                       fir_filter: [1x1 struct]
             fir_separable_filter: [1x1 struct]
                fourier_transform: [1x1 struct]
                 gammatone_filter: [1x1 struct]
            gammatone_filter_bank: [1x1 struct]
                 gmm_nonlinearity: [1x1 struct]
                hilbert_transform: [1x1 struct]
                    infer_respavg: [1x1 struct]
                  int_fire_neuron: [1x1 struct]
            inter_spike_intervals: [1x1 struct]
               likelihood_poisson: [1x1 struct]
                 lindeberg_filter: [1x1 struct]
               lindeberg_spectral: [1x1 struct]
              load_SNS_from_baphy: [1x1 struct]
       load_stim_resps_from_baphy: [1x1 struct]
             load_stim_resps_wehr: [1x1 struct]
               mean_squared_error: [1x1 struct]
               mutual_information: [1x1 struct]
                neural_statistics: [1x1 struct]
                      nim_wrapper: [1x1 struct]
                     nonlinearity: [1x1 struct]
      nonparm_filter_nonlinearity: [1x1 struct]
                     nonparm_gain: [1x1 struct]
             nonparm_nonlinearity: [1x1 struct]
          nonparm_nonlinearity_2d: [1x1 struct]
               normalize_channels: [1x1 struct]
                         passthru: [1x1 struct]
                       pole_zeros: [1x1 struct]
                       pz_synapse: [1x1 struct]
                       pz_wavelet: [1x1 struct]
            pz_wavelet_filterbank: [1x1 struct]
                   reparameterize: [1x1 struct]
                   smooth_respavg: [1x1 struct]
    sparse_empirical_nonlinearity: [1x1 struct]
                      spike_times: [1x1 struct]
          spike_triggered_average: [1x1 struct]
                     split_signal: [1x1 struct]
               state_space_diffeq: [1x1 struct]
               subsample_channels: [1x1 struct]
                       sum_fields: [1x1 struct]
              sum_vector_elements: [1x1 struct]
                    truncate_data: [1x1 struct]
                     unify_signal: [1x1 struct]
                  weight_channels: [1x1 struct]
\end{verbatim}

As you can see, there are already quite a few modules available for neural analysis. You can also see that two modules are still under development and contain syntax errors, so they were not loaded or are available for use.. 

\subsection{A Minimal Module}

Let's take a look at how to make a minimalist module that adds 11 to every number it gets as an input. 

\begin{verbatim}

function m = my_module(args)
% A minimal NARF module. 
% Returns a function module 'm' which implements the MODULE interface.

% Module fields that must ALWAYS be defined
m = [];
m.mdl = @my_module;
m.name = 'my_module';
m.fn = @do_my_module;
m.pretty_name = 'My Adder Module';
m.editable_fields = {'input', 'time', 'output', 'add_parameter'};
m.isready_pred = @isready_always;

% Module fields that are specific to THIS MODULE
m.input  = 'stim';
m.time   = 'stim_time';
m.output = 'prediction';
m.add_parameter = 11;

% Optimize this module for tree traversal  
m.required = {'stim', 'stim_time'};   % Signal dependencies
m.modifies = {'prediction'};          % These signals are modified

% Optional things which may or may not be present
m.is_perf_metric = false;
m.fit_fields = {'add_amount'};

% Overwrite the default module fields with arguments 
if nargin > 0
    m = merge_structs(m, args);
end

% Optional plotting fields
m.plot_fns = {};
m.plot_fns{1}.fn = @do_plot_inputs;
m.plot_fns{1}.pretty_name = 'Plot Output vs Time';

% Finally, define any functions specific to this module
function x = do_my_module(mdl, x)   
    % Add some amount to every single input value
    x.(mdl.output) = x.(mdl.input) + mdl.add_parameter;
end

function do_plot_inputs_and_pnorm(sel, stack, xxx)
    [mdls, xins, xouts] = calc_paramsets(stack, xxx(1:end-1)); 
    do_plot(xouts, mdls{1}.time, {mdls{1}.input1, mdls{1}.input2}, ...
            sel, 'Time [s]', 'Output [-]');
end

end
\end{verbatim}

It should be apparent to most programmers that a module is simply a function that returns a struct named \matlab{m} that has some field/value pairs. \matlab{m} is a default object which will be created even when \matlab{my\_module} is not given any arguments. What may be less obvious -- perhaps confusing, or even mind-blowing for those new to functional programming -- is that the \matlab{args} variable is also intended to be a struct and that \matlab{m.mdl} stores a reference to the \matlab{my\_module} function itself. This means that a NARF module \matlab{m} can create a mutated copy of itself by passing a struct to the function stored under \matlab{m.mdl}. Consider the following example, which assumes that you have already scanned for modules and placed them in the global \matlab{MODULES} structure.

\begin{verbatim}
>> global MODULES;
>> m1 = MODULES.my_module;
>> m1.input

ans =

stim

>> m2 = m1.mdl(struct('input', 'foo'));
>> m2.input

ans =

foo

>> m1

m1 = 

                mdl: @my_module
               name: 'my_module'
                 fn: @my_module/do_my_module
        pretty_name: 'My Adder Module'
    editable_fields: {'input'  'time'  'output'  'add_parameter'}
       isready_pred: @isready_always
              input: 'stim'
               time: 'stim_time'
             output: 'prediction'
      add_parameter: 11
     is_perf_metric: 0
         fit_fields: {'add_amount'}
           plot_fns: {[1x1 struct]}

>> m2

m2 = 

                mdl: @my_module
               name: 'my_module'
                 fn: @my_module/do_my_module
        pretty_name: 'My Adder Module'
    editable_fields: {'input'  'time'  'output'  'add_parameter'}
       isready_pred: @isready_always
              input: 'foo'
               time: 'stim_time'
             output: 'prediction'
      add_parameter: 11
     is_perf_metric: 0
         fit_fields: {'add_amount'}
           plot_fns: {[1x1 struct]}
\end{verbatim}

As you can see, both module instances \matlab{m1} and \matlab{m2} are identical except for the \matlab{input} field. This ability to quickly copy, modify, and perturb module instances is a useful feature to have. If desired, modules can be written to ensure that changing one parameter will necessarily update other, related parameters. For example, if \matlab{add\_parameter} were not a scalar but a vector, you might want it to modify another parameter called \matlab{average} which is based upon the average of that vector. 

\subsection{Required Module Fields}

Modules are just structs; what makes them more than just a struct is that they implement the module \definition{interface}. Every module \emph{must} have these required fields which are used by NARF to interface with the functionality of the module:

\begin{description}
\item [\matlab{m.pretty\_name}] An alphanumeric string describing the function that is displayed to the user of the NARF GUIs. 

\item [\matlab{m.name}] The name of the function. It should be the same as the module function file name. 

\item [\matlab{m.mdl}] A function handle of the module. This will be the same as \matlab{name}, but with an \matlab{@} symbol prefixed to get the function handle. 

\item [\matlab{m.fn}] A handle to a function which does the core computation $f$ associated with the module. Function $f$ should accept four arguments \matlab{(mdl, x, stack, xxx}), where \matlab{mdl} is the module instance struct itself and \matlab{x} is the data input $x$ that it should work on. Although it is rare to use them in practice, for generality a copy of the complete \matlab{STACK} variable and the \matlab{XXX} variable are also provided to the function so that a function $f_n$ can look at parameters or data of modules that preceeded it (e.g. $\phi_{n-2}$, $x_{n-5}$).

\item [\matlab{m.editable\_fields}] A cell array of strings that list which parameters should be editable in the GUI. Sometimes you don't want to expose all of the module parameters to naive users, and so by default no parameters are gui-editable unless listed in this cell array. Any fields listed in \matlab{m.editable\_fields} must have values with one of the following types: integer, floating point, 1D or 2D matrix, string, boolean, function, or a cell arary of strings. No other data types are allowed. If you need to add one, please make sure it is compatible with \matlab{write\_readably.m}.

\item [\matlab{m.isready\_pred}] The ``module-is-ready-to-be-appended predicate''; in other words, a function that returns true only when the module has its data dependencies and connectivity met. Not all modules can be appended to an existing \matlab{STACK}; sometimes the signals are not ready or the input dimensionality is not correct for the module being used. In current practice, this predicate is not used much because we are mostly just building models whose modules all interact harmoniously, but leaving this hook intact is a good idea because perhaps in the future we will need to auto-generate models from modules that are not always compatible. Did I mention that research usually means you leave things in the code that may not be useful anymore, rather than being fastidious about keeping your code clean?

\end{description}

Modules may also have these optional fields which enable special behavior. These aren't fully documented, but you should get the idea by looking at example source code when you need to use them.

\begin{description}

\item [\matlab{m.plot\_fns}] A cell array of functions two fields: plot names, and functions with three arguments each. The first element in each sub-cell-array is a user-readable string naming the plot function, the second is a function handle to a function which plots on the default plot axes, and the third is a struct which contains parameters that will be passed to the plot function.  

\item [\matlab{m.plot\_gui\_create\_fn}] If defined, during GUI creation this function is called and used to create GUI widgets in the designated widget space for each module. If desired, you can then make the module's plots react to changes in the gui widgets. 

\item [\matlab{m.auto\_plot}] If this is defined, when the model has been successfully fit and it is time to generate a summary image of the model, any function placed here will be called.

\item [\matlab{m.auto\_init}] A function that is called which can be used to initialize a module's parameters based on the run-time values of the \matlab{XXX} struct. This is used sparingly but usefully in practice, since it can allow modules to adapt themselves to the particular model structure already defined. It is intended to be called by \matlab{append\_module.m}. 

\item [\matlab{m.is\_perf\_metric}] When true, this signals to any fitting algorithms that this module contains a performance metric that should be used to fit. 

\item [\matlab{m.requires}] This is a list of all the signals that the pure function of the module uses to compute its outputs. If you define this value and put the correct signal in this list, you enable the module evaluator to know when the output of the module has been invalidated by a changing input. In a complex model with many modules, it can save significant simulation time if you only recompute modules whose input signals dependencies have changed, and skip recomputing the rest. 

\item [\matlab{m.modifies}] This is a list of all the output signals of the module. This lets the module evaluator understand that subsequent modules -- who may depend on these output signals -- may have to have to be recomputed after this module. 

\end{description}


\subsection{\matlab{STACK} Global Data Structure}

The \matlab{STACK} data structure is a cell array of cell arrays. The reason for this nesting will be described further in section \ref{sec:parameter-sets}, but for now we need only concern ourselves with the first level of cell arrays, which will have as many elements as there are modules in the stack. 

\subsection{\matlab{META} Global Data Structure}

A model is more than just a collection of modules; certain pieces of information don't really work as a parameter of any particular module, yet need to be carried around with the model. The \matlab{META} struct is used for this purpose, as it contains all the \definition{metadata} of the model, for example:

\begin{itemize}
  \item the name of the model
  \item the filename of the saved model
  \item the path where the saved model file is located
  \item the saved images are associated with the model
  \item when its parameters were fit
  \item how long it took to fit the model, in seconds
  \item which performance metric is used to fit the model
  \item the git hash referring to the version of NARF used to fit the model
  \item other performance metrics
  \item programmer notes on the model
\end{itemize}

It might look something like this:

\begin{verbatim}
>> global META
>> META

META = 

     git_commit: '7c42b9ee3cee2c5f1065ea76157d59179479e139'
          batch: 241
      modelname: 'env100_log2b_firno_initrc_nonl_mse_boost'
      modelfile: '241_por019b-a1_env100_log2b_firno_initrc_nonl_mse_boost.mat'
      modelpath: [1x103 char]
    perf_metric: @pm_mse
       fit_time: 217.1380
\end{verbatim}

\subsection{\matlab{NARFGUI} Global Data Structure}

This will be left undocumented, since it is mostly used by \matlab{narf\_modelpane.m}. For the curious, it contains handles to all of the GUI widgets created for each module. 























\subsection{\matlab{XXX} Global Data Structure}

  A cell array, with the most recent data being first. The contents of each cell could technically be any MATLAB data structure, but quite a few conventions have developed as NARF has evolved. The following are typical values and data structures being used. Feel free to define your own fields as necessary.
 
  \textbf{WARNING: THIS DOCUMENTATION IS HORRIBLY OUTDATED SO DON'T TRUST ANYTHING PAST THIS POINT.}

\begin{tabular}{|l|l|l|l|l|}
SYMBOL  & DESCRIPTION   & TYPE    & SET OR MODIFIABLE BY         &   \\
\matlab{X{}.dat.().cellid         } & Name of the cellid                                           & String  & -                            &   \\
\matlab{X{}.dat.().stimfile       } & Name of the stimfile                                         & String  & -                            &   \\
\matlab{X{}.dat.().include\_prestim} & Boolean. 1 prestim was included, 0 otherwise                 & Boolean & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_stim\_fs    } & Raw stimulus frequency                                       & Double  & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_resp\_fs    } & Raw response frequency                                       & Double  & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_stim       } & Raw stimulus                                                 & [??]   & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_stim\_time  } & Time vector for stimulus                                     & [??]   & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_resp       } & Raw spike timings                                            & [??] & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_resp\_time  } & Time vector for response                                     & [??]   & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_isi        } & Raw inter-spike intervals                                    &         &                              &   \\
\matlab{X{}.dat.().pp\_stim        } & Preprocessed stim                                            &         &                              &   \\
\matlab{X{}.dat.().ds\_stim        } & Downsampled, preprocessed stim                               &         &                              &   \\
\matlab{X{}.dat.().ds\_stim\_time   } & Time vector for downsampled stimulus                         &         &                              &   \\
\matlab{X{}.dat.().lf\_stim        } & Linear filtered stimulus (FIR or whatever)                   &         &                              &   \\
\matlab{.lf\_preds                 } & Needs to be RENAMED                                          &         &                              &   \\
\matlab{X{}.dat.().nl\_stim        } & Nonlinearly scaled stimulus                                  &         &                              &   \\
\matlab{X{}.dat.().pred           } & Prediction of the model &         &                              &   \\
\end{tabular}

In the above, dimensions are indicated with:

\begin{description}
  \item {T} = Time index 
  \item {S} = Stimulus index \#
  \item {C} = Channel index \#
  \item {R} = Repetition index \#
\end{description}

Coefficients are the only things that AREN'T with the time index in first place. Why? Well, it's easier to edit them in the gui if they are not?

\section{Overview of Typical Operations}

The following is a rough overview of how typical operations affect the \matlab{STACK} and \matlab{XXX} structs. 

\begin{description}
\item {Evaluation: } Essentially, there is a chain of function calls, with the output of one function pushed onto the inputs of the next. Mathematically, it's easy to understand: \matlab{XXX\{i+1\} = STACK\{i\}.fn(XXX\{i\})}
\item {Invalidation: } If any intermediate parameter struct is modified, then it erases all \matlab{XXX} cells after it and the computation must recommence from that point. 
\item {Multiple models: } If you need to do different 'branches' of computation or compare model structures, you can store the current computation \matlab{STACK} and save them somewhere, then load them back later.
\item {Module loading: } The only functions available are isted in the "modules" directory, which is read ONCE, at startup. They are only available from the popup selection when their ready\_pred() function returns a true. 
\item {Editing: } Fields in the \matlab{editable\_fields} struct are GUI editable, but most other fields should be editable by passing the argument to the user-defined module creation funciton. However this is just a convention and relies on module authors to implement.
\item {Graphing: } Each module has (multiple) associated graphing functions which can be seleceted via a dropdown.
\item {Error handling: } Whenever you load or run a user-loadable function, a try-catch block around it to catch errors.
\item {Saving and loading: } When you want to save a model, just save the \matlab{STACK} data structure somewhere along with the GIT hash tag and initial data. Data from that point can always be reconstructed. When you want to load a model, loop through the \matlab{STACK} structure, starting from the first data X, and reconstruct the data as you go along.
\item {Optimization pack/unpack: } Packing operations go through \matlab{STACK} sequentially, pulling out any args found in the \matlab{fit\_fields} cell array and putting them into a vector. The unpacking operation goes through \matlab{STACK} sequentially, pushing in any args with a FIT checkbox (accepts a vector as the input). TODO: During optimization, all controls should probably be disabled to avoid invalidation problems?
\item {Optimization} There will need to be three types of modules: a performance metric, termination condition, and a sampling algorithm which picks out the next point to try. These are not part of the model explicitly, and must be reusable amongst different optimization algorithms. 
\end{description}

\section{Allowing Varying Signal Dimensionality}

There is an intrinsic problem in science in that we often do not know how many dimensions we will need to test before starting an experiment. For example, when presenting an auditory stimulus during an experiment, there may be multiple dimensions to the stimulus which need to be indexed:

\begin{enumerate}
\item The amplitude of the sound as a function of time
\item Which experiment the data came from 
\item Which particular sound that was played (i.e., a particular stimuli fragment, usually given an index number)
\item Whether the sound is of one type or another type, such as during discrimination tasks. 
\item The index number of the speaker playing the stimuli
\item The L/R index of which ear the animal is listening with
\item The repetition index number, if a stimulus somehow varies in a minor way across each major stimulus pattern.
\end{enumerate}

For visual presentations of images, the problem can become even worse due to the higher dimensionality of image stimuli, although this depends on how the video stimuli is represented as data, and whether or not each pixel has an X component, a Y component, and RGB color dimensions. 

There are similar problems with dimensionality in regards to model complexity. It is often the case that there will be extra dimensions internal to the model used to help predict neural response to a stimulus. For example, a model may have:

\begin{enumerate}
\item Linear or nonlinear combinations of input data channels intended to model higher order terms. 
\item A filter bank applied to a single stimuli, converting a single stimulus into many.
\item Multiple macro channels of FIR filters, acting on different portions of the signal 
\end{enumerate}

Whether changing dimensionality results from changes to model structure or from the experiment itself, the problem is the same: it is hard to perfectly predict when extra dimensions will need to be added or removed, especially when many models must be tried.

Allowing arbitrary NARF modules to work with every possible experimental dimensionality is a very difficult problem to solve and would require some sort of clever tensor notation. This turned to be too difficult to implement...so it wasn't. Instead, things were just coded up as 3D or 4D matrices. You may have to tweak your data to make it fit into the expected interface of another module; that's just life. 

\section{Model Auto-Initalization Not Encouraged}

One lazy design decision made was not to pass the \matlab{STACK} and \matlab{XXX} as arguments to each module creation function. Although this could easily be done to allow modules to automatically initialize themselves, most of the time we want to set their values via a script anyway. Initializing a module from more than one place could lead to confusion about who sets the last, final value of the module, so this approach was avoided.

\section{Example Optimization}

Clearly optimization is clearly NOT part of the model, yet we will want to plot various quantities and act in a modular way in a similar fashion to how the model already works. Pluggable sampling, termination conditions and objective functions are needed. 

\section{Further Development}

See the TODO.org file in NARF's main directory.

\chapter{NARF GUIs}

Not much to say here; \matlab{narf\_browser.m}, \matlab{narf\_analysis.m} and \matlab{narf\_modelpane.m} are the big three. 

\chapter{The Keyword Composition System}

Keywords are arbitrary functions. The concept is similar to how the STACK works, but intermediate values are not preserved. 

\chapter{Optimization: Fitters \& Performance Metrics}

Fitters go in the \matlab{fitters/} directory. 

\chapter{Distributed Computation}

\chapter{Advanced Functionality}

\section{Parameter Sets}

\section{Jackknifing}



\chapter{List of Modules}

* FOREVER UNSOLVED ARCHITECTURAL PROBLEMS
  1. Modifying a module (adding new fields) breaks compatability with previous versions.
  2. No way of loading two modules simultaneously.
  3. When a clustered thing crashes, I lose intermediate results. 


\end{document}
