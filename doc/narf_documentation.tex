\documentclass{article}
\newenvironment{qanda}{\setlength{\parindent}{0pt}}{\bigskip}
\newcommand{\Q}{\bigskip\bfseries Q: }
\newcommand{\A}{\par\textbf{A:} \normalfont}
\newcommand{\definition}[1]{\textbf{#1}}
\newcommand{\matlab}[1]{\texttt{#1}}

\title{Neural Activity Regression Framework (NARF)}
\author{Ivar Thorson}
  
\begin{document}

\maketitle

This document was written to clarify and document design decisions made regarding the Neural Activity Regression Framework (NARF) implemented in the Brain, Hearing and Behavior Laboratory of the Oregon Health and Science University (OHSU), under the supervision of Prof. Stephen David. As such, it is intended for developers rather and is very much a work in progress. 

\section{Problem and Approach}

\subsection{The Problem}

There are many possible mathematical models which could be used describe the spiking activity of a neuron given some sensory stimulus. In our case, we will be considering mostly models of neurons in the auditory cortex of a mammal, usually a ferret, and usually near the 'A1' region. Neurons in this region of the brain often respond preferentially to a particular auditory frequency. For example, some neurons may spike often whenever a sound with accoustic energy near 1.2kHz is heard. Another neuron may spike frequently whenever a 28kHz tone is heard. Our goal is to play recordings of various sounds, measure neural activity, and attempt to discover as much as we can about the neuron(s) we are studying.

Mathematically, the problem is that of developing a mathematical model which can accurately predict how a neuron will fire when a new, as-yet-unheard sound is played. This is a classic inference problem, in which we seek to infer which system produced the neural response that we measured. If the model's prediction closely matches the measured response of the neuron, then we have learned something significant about the neuron. The better our ability to predict the neuron's activity, the better our understanding of it, and the more we may learn through experiment about how neural activity is affected by various factors. Our ability to learn about the dynamics of neurons through experiment is thus closely related to our ability to model each neuron.

Fortunately or unfortunately, while the neuroscientist community has developed a plethora of mathematical models which can describe this behavior, there remain few guidelines on which model should be used or how model parameters should be found. The difficulty for most scientists is in the implementation of existing models; it is often a significant amount of work to adapt another person's model to your needs, and significantly more work to develop one yourself. Ideally, we would have some way of fitting several different models to the measured activity of the neuron, so that we might quickly identify the characteristics of the neuron under study. It would also be advantageous to be able to recombine parts of the model which were particularly useful in describing neural activity such that variations on successfully predictive models could be quickly examined. 

\subsection{NARF's Approach: Pure Function Composition}

While there can likely never be a completely general solution to the neural model fitting problem, borrowing well-developed concepts from mathematics can at least get us part of the way there. If we consider each aspect of a mathematical model to be a \definition{pure function} $f_1$ which accepts an argument $x_1$ and returns a value $x_2$, we can write it as

\begin{equation}
  f_1(x_1) \rightarrow x_2
\end{equation}

The notion of a 'pure' function is simply that it will always produce the same result when given the same input. In the domain of computer science, this necessarily means that there are no side effects; no print statements, saving to disk, and no use of global variables. If a function's computation is based in any way on a global variable, then it is possible for the same function, given the same arguments, to produce different results depending on the value of the global variable at the time it was run. This is in general a very bad way to write programs, and can be very confusing. 

If we make our model out of more than one pure function (e.g. $f_1, f_2, ..., f_n$), then we could potentially chain the output of one function to the input of the next:

\begin{eqnarray*}
f_{1}(x_{1}) & \rightarrow & x_{2}\\
f_{2}(x_{2}) & \rightarrow & x_{3}\\
 & \vdots\\
f_{n}(x_{n}) & \rightarrow & x_{n+1}
\end{eqnarray*}

And we will always get the same output ($x_{n+1}$) at the end of the computational chain. In a perhaps more familiar notation we could have written

\begin{equation}
x_{4} = f_3(f_2(f_1(x_1)))
\end{equation}

All of the above has assumed that every function has no parameters. If we had a function which added 23 to it's input argument and then divided by three ($f_1(x_1) = (f_1 + 23) / 3 $), then we would be required to define a completely new function $f_1$ if we wanted to try adding 25 or dividing by a different number. If some part of the function's behavior will be changing regularly and many values must be tried, then it makes sense to bundle these changing parts into a model parameter vector $\phi_1 = [\phi_{11} \phi_{12}$. 

Now each function should look like this:

\begin{equation}
  f_1(x_1, \phi_1) \rightarrow x_2
\end{equation}

And we chain them together as before:

\begin{eqnarray*}
f_{1}(x_{1}, \phi_1) & \rightarrow & x_{2}\\
f_{2}(x_{2}, \phi_2) & \rightarrow & x_{3}\\
 & \vdots\\
f_{n}(x_{n}, \phi_n) & \rightarrow & x_{n+1}
\end{eqnarray*}

Now we can describe nearly any neural model by specifying some initial conditions $x_1$, a list of functions $f_1, f_2, ..., f_n$, and a list of parameter vectors $\phi_1, \phi_2, ..., \phi_n$. By executing the functions one after another, our prediction about the neural activity will finally be held in $x_{n+1}$. If we want, we can observe intermediate values of $x$ to better understand how the model is functioning as a whole. 

The bundling of $f$ and $\phi$ together is tentatively being called a \definition{module}, for lack of a better way to describe each block of modeling functionality. 

\section{Module Technical Details}

MATLAB's data types prohibit us from being completely general with regard to how we allowing functions $f$ to do almost anything and how we use data $x$ to represent almost anything. The best that we can do is to choose MATLAB's cell arrays in both cases, because of their generality and default behavior of reference-passing.

The structure which holds the list of functions $f_1, f_2, ...$ is therefore called \matlab{STACK} in weak analogy to the traditional computer data structure, and the structure which holds all the $x_1, x_2, ..., x_{n+1}$ is called \matlab{XXX} in weak analogy to the fact that each $x$ is dependent upon the previous. Each cell of \matlab{STACK} and \matlab{XXX} is a structure containing named fields. You may put any fields you like in each cell, except for the fields defined in the next two sections, which have special meaning.

\subsection{STACK data structure}

  A cell array containing the functions and their parameters that were applied to reach this point. 

\begin{tabular}{l|p{4in}}
\matlab{STACK{}.name} & Function file name \\
\matlab{STACK{}.fn}             &  The function handle\\
\matlab{STACK{}.pretty\_name}    &  User-readable pretty function name\\
\matlab{STACK{}.plot\_fns}       &  Struct array with fields \matlab{pretty\_name} and \matlab{fn}  \\
\matlab{STACK{}.editable\_fields} & Fields that may be user edited via the GUI.\\
\matlab{STACK{}.isready\_pred}    &  A predicate function that is passed (STACK, X) and returns true iff it's ready to run.\\
\matlab{STACK{}.gh}              & The "Gui Handles" structure, to be described in TODO.\\
\end{tabular}

\subsection{Module Fields}

If \textbf{F()} is a function to be used in the NARF architecture, it must satisfy a few core requirements to be used properly. These features are:
\begin{enumerate}
\item \textbf{F()} must be placed in a directory where NARF can find it. 
\item \textbf{F()} must return a default struct \textbf{m}
when given zero arguments. The structure \textbf{m} must satisfy
the following requirements:

\begin{enumerate}
\item It \emph{must }have these required fields:

\begin{description}
\item [{m.pretty\_name}] An alphanumeric string describing the function
\item [{m.fn}] A handle to a function which does the 'core computation'
of \textbf{F(). }It accepts two arguments (\textbf{p}, \textbf{x}).
\textbf{p} is a copy of the \textbf{m} structure so that the
state of\textbf{ F()} may be easily read. \textbf{x} is a data structure
passed from a previous function.
\item [{m.input\_validation\_fn}] A function which accepts a two arguments
(\textbf{p},\textbf{ x}), and returns a true if \textbf{m.fn(p,x)}
can be run, otherwise it returns an error string.
\item [{m.editable\_fields}] A cell array of field names. These will
be displayed to the GUI user for easy editing. 
\end{description}
\item It \emph{may} also have these optional fields which enable special
behavior:

\begin{description}
\item [{m.plot\_fns}] A cell array of cell arrays with three elements
each. The first element in each sub-cell-array is a user-readable
string naming the plot function, the second is a function handle to
a function which plots on the default plot axes, and the third is
a struct which contains parameters that will be passed to the plot
function. 
\end{description}
\item Any fields listed in \textbf{m.editable\_fields} must have values
with one of the following types: integer, floating point, 1D or 2D
matrix, string, boolean, or cell array of strings. No other data types
are allowed.
\item The user is allowed to define any other field in \textbf{m}
that they wish and store arbitrary data in it. However, if it is not
listed in \textbf{m.editable\_fields}, it will not ever be directly
editable.
\end{enumerate}
\item When \textbf{F() }is given a single argument args, it must:
(Footnote: a useful function to accomplish these two things is update\_narf\_struct.m)

\begin{enumerate}
\item Check in args for the existence of fields listed in .
\item Any fields that exist in args will be copied into m
m to overwrite the default values of m.
\end{enumerate}
\end{enumerate}

\subsection{XXX data structure}

  A cell array, with the most recent data being first. The contents of each cell could be anything. The following are typical values being used, but these are only by convention. Feel free to define your own fields as necessary.
 
\begin{tabular}{|l|l|l|l|l|}
SYMBOL  & DESCRIPTION   & TYPE    & SET OR MODIFIABLE BY         &   \\
\matlab{X{}.dat.().cellid         } & Name of the cellid                                           & String  & -                            &   \\
\matlab{X{}.dat.().stimfile       } & Name of the stimfile                                         & String  & -                            &   \\
\matlab{X{}.dat.().include\_prestim} & Boolean. 1 prestim was included, 0 otherwise                 & Boolean & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_stim\_fs    } & Raw stimulus frequency                                       & Double  & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_resp\_fs    } & Raw response frequency                                       & Double  & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_stim       } & Raw stimulus                                                 & [SxN]   & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_stim\_time  } & Time vector for stimulus                                     & [1xN]   & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_resp       } & Raw spike timings                                            & [SxMxR] & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_resp\_time  } & Time vector for response                                     & [1xM]   & load\_stim\_resps\_from\_baphy.m &   \\
\matlab{X{}.dat.().raw\_isi        } & Raw inter-spike intervals                                    &         &                              &   \\
\matlab{X{}.dat.().pp\_stim        } & Preprocessed stim                                            &         &                              &   \\
\matlab{X{}.dat.().ds\_stim        } & Downsampled, preprocessed stim                               &         &                              &   \\
\matlab{X{}.dat.().ds\_stim\_time   } & Time vector for downsampled stimulus                         &         &                              &   \\
\matlab{X{}.dat.().lf\_stim        } & Linear filtered stimulus (FIR or whatever)                   &         &                              &   \\
\matlab{.lf\_preds                 } & Needs to be RENAMED                                          &         &                              &   \\
\matlab{X{}.dat.().nl\_stim        } & Nonlinearly scaled stimulus                                  &         &                              &   \\
\matlab{X{}.dat.().pred           } & Prediction of the model &         &                              &   \\
\end{tabular}

In the above, dimensions are indicated with:

\begin{description}
  \item {S} = sound stimulus index \#
  \item {R} = repetition index \#
  \item {N} = Time index at the sampling rate of the stimulus. 
  \item {M} = Time index at the sampling rate of the response
  \item {T} = Time index in downsampled frequency
  \item {F} = Preprocessing index \#
\end{description}


Coefficients are the only things that AREN'T with the time index in first place. Why? Well, it's easier to edit them in the gui if they are not?

\section{Overview of Typical Operations}

The following is a rough overview of how typical operations affect the \matlab{STACK} and \matlab{XXX} structs. 

\begin{description}
\item {Evaluation: } Essentially, there is a chain of function calls, with the output of one function pushed onto the inputs of the next. Mathematically, it's easy to understand: \matlab{XXX\{i+1\} = STACK\{i\}.fn(XXX\{i\})}
\item {Invalidation: } If any intermediate parameter struct is modified, then it erases all \matlab{XXX} cells after it and the computation must recommence from that point. 
\item {Multiple models: } If you need to do different 'branches' of computation or compare model structures, you can store the current computation \matlab{STACK} and save them somewhere, then load them back later.
\item {Module loading: } The only functions available are isted in the "modules" directory, which is read ONCE, at startup. They are only available from the popup selection when their ready\_pred() function returns a true. 
\item {Editing: } Fields in the \matlab{editable\_fields} struct are GUI editable, but most other fields should be editable by passing the argument to the user-defined module creation funciton. However this is just a convention and relies on module authors to implement.
\item {Graphing: } Each module has (multiple) associated graphing functions which can be seleceted via a dropdown.
\item {Error handling: } Whenever you load or run a user-loadable function, a try-catch block around it to catch errors.
\item {Saving and loading: } When you want to save a model, just save the \matlab{STACK} data structure somewhere along with the GIT hash tag and initial data. Data from that point can always be reconstructed. When you want to load a model, loop through the \matlab{STACK} structure, starting from the first data X, and reconstruct the data as you go along.
\item {Optimization pack/unpack: } Packing operations go through \matlab{STACK} sequentially, pulling out any args found in the \matlab{fit\_fields} cell array and putting them into a vector. The unpacking operation goes through \matlab{STACK} sequentially, pushing in any args with a FIT checkbox (accepts a vector as the input). TODO: During optimization, all controls should probably be disabled to avoid invalidation problems?
\item {OPTIMIZATION} There will need to be three types of modules: a performance metric, termination condition, and a sampling algorithm which picks out the next point to try. These are not part of the model explicitly, and must be reusable amongst different optimization algorithms. 
\end{description}

\section{Allowing Varying Signal Dimensionality}

There is an intrinsic problem in science in that we often do not know how many dimensions we will need to test before starting an experiment. For example, when presenting an auditory stimulus during an experiment, there may be multiple dimensions to the stimulus which need to be indexed:

\begin{enumerate}
\item The amplitude of the sound as a function of time
\item Which experiment the data came from (ie, stimfile)
\item Which particular sound that was played (ie, a particular stimuli fragment, usually given an index number)
\item Whether the sound is of one type or another type, such as during discrimination tasks. 
\item The index number of the speaker playing the stimuli
\item The L/R index of which ear the animal is listening with
\item The repetition index number, if a stimulus somehow varies in a minor way across each major stimulus pattern.
\end{enumerate}

For visual presentations of images, the problem can become even worse due to the higher dimensionality of image stimuli, although this depends on how the video stimuli is represented as data, and whether or not each pixel has an X component, a Y component, and RGB color dimensions. 

There are similar problems with dimensionality in regards to model complexity. It is often the case that there will be extra dimensions internal to the model used to help predict neural response to a stimulus. For example, a model may have:

\begin{enumerate}
\item Linear or nonlinear combinations of input data channels intended to model higher order terms. 
\item A filter bank applied to a single stimuli, converting a single stimulus into many.
\item Multiple macro channels of FIR filters, acting on different portions of the signal 
\end{enumerate}

Whether changing dimensionality results from changes to model structure or from the experiment itself, the problem is the same: it is hard to perfectly predict when extra dimensions will need to be added or removed, especially when many models must be tried.

Allowing arbitrary NARF modules to work with every possible experimental dimensionality is a very difficult problem to solve. Several possibilities for enabling varying dimensionality include the following.

\begin{enumerate}
  \item One possibility is to flatten all dimensions into a 2D matrix, with the first dimension being time and the second being the combination of all other dimensions, flattened or indexed in an appropriate way. The benefits of this approach are simplicity and easy combination of several dimensions, as well as a uniform interface. Drawbacks of this approach include a difficulty in finding index numbers, and difficulty when concatenating or trimming indexes along a particular dimension.
  \item Another possibility is to just keep nesting indexed data structures until a 2D matrix is reached. The benefit of the approach is a logical, easy-to-understand data structure, however it is difficult to arbitrarily index such a data structure without complex loops. 
  \item Another possibility is to not store all stimulus data in the same matrix, and instead give field names to every quantity. The benefit of this approach is simplicity, but the drawback is that there will be an explosion in the number of field names and it will be difficult to provide a standardized interface . 
  \item Another possibility is to avoid the problem entirely and allow modules which are incompatible; some work on 2D matrices, others on 3D matrices, etc. The main drawback would be a large amount of duplicated functionality across different interfaces. 
  \item Another possibility is to provide ``interfacing'' functions which act as adapters and chunk out data from an N diemnsional matrix as needed by the module at work. 
\end{enumerate}

The solution taken in NARF was a combination of the first and last possibilities. In other words, to flatten stimulus data into to a 2D matrix in which time index is the first dimension, and the second dimension is the enumerated combination of other dimensions. A simple indexing functionality to help you select which columns are of interest for a particular operation. If data is concatenated or new dimensions are adding, these indexing functions must be rebuilt. 

UPDATE: This doesn't actually work out nicely because any graphing function needs to know about the dimensionality in order to select a single trace. If you make an FIR filtering function work across every dimensionality (which is possible with the above nifty interface), you still can't graph the traces produced by those since you end up plotting multiple values. You just can't index through them all. This really calls into question the possibility of ever really abstracting the dimensonality...although you could compute stuff, you couldn't graph stuff! Or, unselected dimensions would still be plotted, meaning you could have a large number of traces appearing. 


\section{Model Auto-Initalization Not Encouraged}

One lazy design decision made was not to pass the \matlab{STACK} and \matlab{XXX} as arguments to each module creation function. Although this could easily be done to allow modules to automatically initialize themselves, most of the time we want to set their values via a script anyway. Initializing a module from more than one place could lead to confusion about who sets the last, final value of the module, so this approach was avoided.

\section{Example Modules}

TODO.

\section{Example Script}

See 'analysis/example\_script.m' for more an example of how a linear model can be trained on an arbitrary data set.

%% The software architecture of NARF is similar to the Generalized Nonlinear Model (GNNM) framework proposed by McFarland, Cui, Butts). 

\section{Example Optimization}

See TODO.org for more details. Clearly optimization is clearly NOT part of the model, yet we will want to plot various quantities and act in a modular way in a similar fashion to how the model already works. Pluggable sampling, termination conditions and objective functions are needed. 

\section{Further Development}

See the TODO.org file in NARF's main directory.

\end{document}

% \subsection{State-space model of cochlear vibration}

% There are three main parts, which for brevity will be referenced using the following mathematical symbols adopted from standard engineering control theory nomenclature:

%% \begin{eqnarray}
%%  \dot{x} & = A(x) + B(u)\\
%%  \dot{y} & = C(x) \\
%% \end{eqnarray}

%% where $x$ is the state of the system, $u$ is ....

%% \begin{description}[]
%%   \item [$A(s)\rightarrow u$] A \definition{filter} $p$ which transforms the time-indexed sound wave $x$ into stimulus intensity vector $u$. It models how the cochlea transforms the stimulus sound wave to the vibration of the basilar membrane. Vector $s$ has several components which may correspond to different filters. 
%%   \item [$q(u,x_{i-1})\rightarrow x_i$] A probably nonlinear \definition{activity function} $q$ which maps stimulus intensity $u$ and previous internal state $x_{i-1}$ to a new activity state $x_{i}$. 
%%   \item [$C(x) \rightarrow y$] A \definiton{spike noise function} describing the probabilistic nature of when spiking is observed. 
%%   \item [$m() \rightarrow z$] A \definiton{performance metric} describing 
%% \end{description}


%% \begin{qanda}

%% \Q How do we find both the model parameters and also the average rate of fire for the neuron? We know neither: our data just shows 'observed' firings from which we infer the true rate.
%% \A Treat both of them as unknowns. Use Expectation Maximization (EM) algorithm, a nonlinear model, and an Unscented Kalman Filter (UKF) to iterate between estimating the best model prameters and re-interpreting the data in light of those parameters until convergence occurs. % TODO: Insert blog diagram figure here.

%% \Q Pearson's r coefficient of correlation is a linear measure -- how can we use it to compare nonlinear systems?
%% \A Use feedback-linearization-style model inversion of the observation system, and we do the comparison in a linearized space. Let's use 'average rate of fire' since that is an easy place to start, and it neatly partitions the system model and observational model.

%% \Q How big should PSTH bins be? Small bins are very discontinuous and large bins smooth so much that significant information is lost. 
%% \A Trick question; let's avoid binning at all and instead use a method, such as a Kalman filter with a Poisson process observation model, which allows use of very small bins and interpolates the spiking rate between measurements, maximizing the information gleaned from the spiking data. PSTHs have other problems: there is no error bounds, they are just data. 

%% \Q Will the EM algorithm and Kalman filters cause phase lag in the model estimation?
%% \A I don't think so, but I've been wrong before.

%% \Q How should we include the preprocessing (like stimulus envelope discovery or gammatone filtering) in our framework?
%% \A Consider the preprocessing to be part of the model itself. That way, all variable parameters can be accomodated in the same fashion.

%% \Q What method will be used to optimize the values for a large number of parameters?
%% \A Several methods could be used: \begin{enumerate}
%%   \item Boosting
%%   \item Simulated annealing
%%   \item Conjugate gradient descent
%% \end{enumerate}

%% \Q What about evaluation of the performance? MUST be cross-validated for sanity.
%% \A \begin{enumerate}
%%   \item Simple correlation
%%   \item Jackknifed correlation (with error bars)
%%   \item N-fold cross validation error
%%   \item Error under L1, L2, Linfty norms
%%   \item Log-likelihood of the model (given some variance)
%%   \item Monte-carlo swarm average (checks log-likelihood of many many points)
%% \end{enumerate} 

%% \Q What is the to the optimization engine?
%% \A Optimization engines need
%%    ARGUMENTS:
%%        An initial parameter vector $\theta$
%%        An initial search variance $\sigma_{\theta}$ expressing some priors about the parameter vector. Setting a prior to variance 0 makes it act like a constant.
%%        A model function $M(\theta) \rightarrow {P, N, Q}$ that creates a model corresponding to those parameters $\theta$, where
%%           $P(\theta) \rightarrow G$ is the prefilter creation function 
%%           $N(\theta) \rightarrow Z$ is a nonlinear, state space model creation function
%%           $Q(\theta) \rightarrow \hat{Z}$ is the inverse observational model creation function
%%        An evaluation function $E(Z,\hat{Z}) \rightarrow r$ which measures the model's goodness of fit to the observed measurements
%%        Optimization state $\psi$, which is just a vector to store whatever relevant values the optimization function needs to make its decisions
%%        An optimization function $O(\theta, \psi, r) \rightarrow \theta$ which improves the estimate of the parameters based on the model performance
%%        A termination condition function $\Omega(\psi)$ which returns a boolean indicating that the optimization is complete. For example, a limit on the number of optimization iterations or a measurement of convergence. 

%% \Q What components must be defined for each model?
%% \A A model of a neuron has three parts ($P,N,Q$ as introduced above above): \begin{enumerate}

%%   \item[PREFILTER:] Turns raw waveform into a pseudo frequency-domain signal (may not be the true linear frequency domain, however). Models the cochlea and mechanotransduction.
%%       A. Gammatone filter (center frequency, bandwidth, phase locked or not, averaging window size)
%%       B. Square wavelet filter, ie DFT. (center frequency, bandwidth, averaging window size)
%%       C. Arbitrary FIR wavelet filter
%%       D. Possibly, a physical model-based representation such as a damped harmonic oscillator.
%%       E. Exotic wavelets: sinc, lanczos, mexican hat, meyer
%%       F. Envelope of the stimulus

%%       Prefilters can be created with functions $P(\theta) \rightarrow G$ which return filter functions $G(X,f_X) \rightarrow $. Functions $G$ have the following properties:

%%        ARGUMENTS:
%%            $X$    1xN vector of the stimulus
%%            $T_X$  Stimulus sample spacing interval [s]
        
%%        RETURNS:
%%            $Z$    1xM vector that is the filter's response
%%            $T_Z$  Filtered stimulus sampling interval [s]
   
%%    Prefilter creation functions $P$ accept model parameters $\theta$ that define precisely how the stimulus is filtered. If you wish to treat this as a hyperparameter, you may set it.

%%    \item [MODEL:] Turns that pseudo frequency domain signal into an average rate of neural activity. Models the neuron. 
%%       A. First order linear model with fixed delay
%%       B. Linear model with a finite impulse response
%%       C. Synaptic depression model. (Filter bank + weights)
%%       D. Multiple synapses with different latencies (Timing + weights)
%%       E. Self-referential ``Tire after firing'' stimulus model: increase threshold as stimulus increases (Refractory period)
%%       F. Log or sqrt compression at input or on output
      
%%    \item [SPIKING/OBSERVATIONAL INVERSE MODEL] Turns the discrete spike events into an estimate of the probabilistic process that generated them. 
%%       A. Poisson spiking with a smoothly varying average rate of fire (what I have now)
%%       B. No model, just binning spike events.
%%       \end{enumerate}

%% \Q What is the interface between data selection and model fitting?
%% \A A ragged vector of vectors of stimuli, X, a matrix of the responses to each stimuli Y, and the sample rates of each. Example:

%%    X = [stim1@t0, stim1@t1, ..., stim1@tn;
%%         stim2@t0, stim2@t2, ..., stim2@tn];
%%    Xfs = [0.01;  % 100Hz
%%          0.02]; % 50Hz
%%    Y = [resp1@t0, ...; 
%%      resp2@t1, ...]; 
%%    Yfs = [0.01; % Sample rate
%%           0.02];

%%    Requirements:
%%        All of the above must have the same number of rows
%%        cols(Y(i)) * Yfs(i) == cols(X(i)) * Xfs(i) must be true for any i

%%    This interface can therefore be used for fitting a model to a single data set, or even to multiple data sets. In the latter case, the data sets cannot be directly concatenated because that would create discontinuities in state that wouldn't fit the model. 

%% \Q What is the interface to the optimization system or systems?
%% \A The optimization function:
%%    ARGUMENTS: 
%%      params      An initial parameter vector $\theta$.
%%      mapfn       A function that returns a model function corresponding to parameters $\theta$. 
%%      modelfn     A function that returns a model function using parameters. 
     
%% \Q How can I map a large number of parameters into a model?
%% \A Define a function for each model that provides a mapping from a vector to a EM-runnable object.
%%    For example, 

%% \Q How can I visualize how well the model is doing?
%% \A Plot the scatter plots, correlation, and best fitting line. Also, plot the residuals!

%% \Q How can I visualize the effect of the parameters?
%% \A Plot the function defined by the parameters and histogram where the data falls on that function.

%% \Q How can I make a simple system for graphing results?
%% \A Flat files loadable by matlabe are probably the easiest. Each data file gets its own folder. Variables get names
%%    stim_fs.mat     Stimulus file at a particular frequency, channel 1
%%    resp_fs.mat     Response file at a particular frequency
%%    envelope.mat    The stimulus envelope at a particular frequnecy.
    
%% \Q How should I structure the model-estimation script to save results incrementally, be interruptable, and incrementally visible?
%% \A Basically, the main script will a file doesn't exist, then a function is called to create it. 
%%     setdatafile()    -> Sets the path to look at a particular directory.
%%     querydb()        -> original data files
%%     run_gammagram()  -> Creates a gramma spectrogram
%%     run_filter()     -> Created filter'd stimulus trace using the model (or envelope funciton)
%%     run_em()         -> Do expectation maximization algorithm (ON EVERY PART OF THE DATAFILE, WHICH MAY BE MULTIPLE EXPERIMENTS!)
%%     run_strf()       -> compute the STRF from convolving the gammagram and response data
%%     run_residuals()  -> compute the residuals
%%     run_bestfit()    -> compute best 2nd order linear fits for correlations of this model
%%     run_plotreport() -> Plots an all-encompassing report of the model

%%     Higher level functions:
%%     run_plotcorr()   -> Plot correlation scatter plots between various models. (to help you hunt for next nonlinearity). 
%%                         Plots signal histogram on diagonal, scatter plots on lower trinagle, residuals on upper triangle. 
%%                         Where to plot residual histograms or residual power spectra?

%% \Q How do I convert a function call with various parameters into a file name, and back again?
%% \A Define a function that converts that into a simple text string, and another that 
%%     run_fn(fn, params)     Returns a string
    
%% \Q What about higher level?
%%      Accepts a function to run
%%      Accepts some arguments
%%      Checks if arguments exist. If not, errors. 
%%      Converts 
%%      Checks if the function results already exist
%%      If not, checks arguments.
   
%% Function to generate new sample points in the unknown space
%% 1. Sample uniformly in the Nx2 vector between bottom a and top limits b
%% 2. Samples according to a gaussian with mean mu and covariance matrix Sig
%% 3.  

%% Checking assumptions are correct:
%%    1. Try a forward model structure with your initial starting model, getting an avg rate. 
%%    2. Plot the distribution of Inter-Spike Intervals, scaling time accordingly.
%%    3. Check the fit of various observational noise distributions against the ISIs
%%    4. Infer what the true state must have been in the context of that noise distribution.
%%    5. Fit model using that smoothed data and EM
%%    6. Do evaluation with the improved model

%% % Interesting algorithm could be: 
%% % Define limits on the space (center + gaussianity)
%% % Partition space into two halves
%% % sample randomly in each, evaluate the best point
%% % Pick the better of the two spaces as your new limits

%% % The gaussian k-means sampling algorithm (May not converge!)
%% % 1. Sample randomly at first, evaluate them.
%% % 2. Sort the samples tested by their scores (Create a matrix, use sortrows() to sort by first column)
%% % 3. Take the first 30 samples or so,
%% % 4. Work out the mean, variance on those top samples
%% % 5. Generate 1000 more samples with that mean and variance, evaluate them.
%% % 6. Loop to 2. 

%% % SCAAT Boosting algorithm
%% %  Possible problems: no guaranteed convergence for nonlinear systems. 
%% %      Randomizing the step size would help avoid loops of death
%% %      Allowing different step sizes for each vector element would help avoid scaling problems where one parameter is 0.001 and the other is 10000.
%% %      Having periodic random sampling of the space could also help
%% %      Having a second loop which samples many many points along the free vector until the optimum value is found, migth also help.

%% \end{qanda}

%%  Extend depression model to include presynaptic calcium concentrations (4 parameter depression/facilitation model)
%%           Presynaptic calcium concentration (and/or synaptic cleft neurotransmitter concentration), a function of presynaptic potential
%% 	  Calcium (and/or synaptic cleft neurotransmitter concentration) recovery rate
%% 	  Vesicle depletion rate as a function of presynaptic calcium concentration
%% 	  Vesicle recovery rate 
%%    3. [ ] Mechanotransduction potassium effects (depression model?)
%%    4. [ ] Nonlinearity of frequency sensitivity over A1 (and presumably middle-ear transfer fn)

%% \begin{itemize}
%% \item Replace the correlation coefficient with the jackknifed version.
%% \item mag007d-d1 	Pen 	12-03-13 	0.719+/-0.044* looks pretty good!
%% \end{itemize}

%% * How could functions be passed from one to the next?
%%   All functions are assumed to accept TWO arguments and return TWO arguments
%%   Those arguments are the \matlab{STACK} data structure and the X data structure

%% * When called with no arguments, each function returns 2 arguments:
%%   1. The 'meta' information about the function
%%   2. The 'params' struct, which specificies the default parameters

%% * What should I do about optimizing parameters for multiple models?
%%   Nothing at all, just yet! But I guess it could be done by iterating across the models as well.

%% * Besides optimization, it would be useful to be able to run the optimization-type thing to get plots which answer certain questions:
%%   Example: what is the effect on correlation as you vary the downsampling frequency, for each of these training groups? (Would be a nice graph)
%%   Example: Does using a certain preprocessing function improve or reduce the accuracy of most models' predictions?

%% * What is narf_modelpane used for?
%%   It was written:
%%   1. To visualize a model's behavior on data
%%   2. To edit a model easily and explicitly
%%   3. To encapsulate all assumptions about the model in a stack
%%   4. To select which parameters are to be optimized with an optimization routine. 
%%      (The optimization routine GETS a copy of the stack, plays around with the data, then SETS the stack again after optimization is complete.)

 
%% * MODULE FUNCTIONALITY
%% ** Preprocessing: Anything that creates the dat.().pp_stim field
%%    The big two filters are an elliptical bandpass and gammatone filters

%% ** Downsampling: Anything that creates the dat.().ds_stim and dat.().ds_stim_time fields.
%%    I decided that downsampling should only occur on the stimulus side, since the response already just be loaded at the frequency that you wish.
%%    If you are just doing simple correlation comparisons, you will want to downsample to the same frequency as your response. 
%%    The same if you are doing some sort of interpolated response comparison, but you will leave your response freq high, and apply a convolution over your response to 'smooth' it a bit.
%%    However, if you are doing ISI comparisons, you will NOT care about your response sampling frequency, and instead compute the ISI times. 
%%    To accomodate all theses cases, downsampling only works on the stimulus side.

%% ** FIR filters
%%    Your free paremeters are the number of coefficients in the filter, and how many filters you want.
%%    Each filter spans all of the input channels. (I think it makes more sense to have one filter which acts across all channels than many filters which only act on one channel each)

%% * Allowed Dimensions: How should can we accomodate the later addition of extra dimensions in the future, such as behavioral characteristics?
%%   Right now we have:
%%   In the future, we may have more. 
%%   The only way I can think about allowing multiple dimensions to vary arbitrarily would be to either:
%%   A) Somehow keep track of their numerical indexes as you go along, using a struct
%%   B) Avoid numerical indexes and use struct arrays or cell arrays everywhere? 
%%   Overall, option A sounds like the more efficient choice

%% * Tricky things:
%%   We may need to do an iteration procedure that treats one part of the model (IE, Linear FIR filters) differently from a nonlinear part (In my opinion, this is just a special case sampler)
%%   If you modify a function after starting up narf_gui, what will happen? (Right now, changes to the pretty-name and params will not be altered without restarting narf_gui, however if you fix the function itself then that is fine.)

%% * Possible refactoring
%%   1. Data ordering is perhaps nonstandard, since we need filter(B,A,X,[],2) instead of filter(B,A,X);
%%   2. Should PREFILTEREDSTIM be a 3D matrix, or is it more convenient to use as a mixture of cell array and 2H matrices.? 
%%      STIM [30x400000] (30 tones with 400000 samples in time each)
%%      RESP [30x400000x3] (3 reps)
%%      PREFILTEREDSTIM{numoffilters} and under each cell [30x400000]
%%   3.  Rewrite of dbchooserawfile() because it's so damn useful for selecting a file, but let's make it work for multiple stimulus files
%%       (Should also display well, site and have selectors for channel, unit, etc
%%   4. Use squeeze() to remove unneeded dimensions from a matrix.
%%   5. Why is it 'stimpath' and 'stimfile' but 'path' and 'respfile'. it should be 'resppath'?
%%   7. Why is loadspikeraster the only thing that cares about the 'options' struct?
%%   8. Where should the line be drawn between analysis in the DB, partitionining things for your search within the DB, holding out data, etc?
