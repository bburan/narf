OBSERVATIONS ON VARIOUS MODELS

* General Model Structure.
  1. MODEL COMPRESSOR: Any nonlinearity before the FIR filter a "compressor". 
  2. FIR FILTER: Has sufficient coefficients to cover the past 100ms of history and span the input dimensions
  3. NONLINEARITY: A global nonlinearity on the output of the FIR filter. 

* Compressors (Jan 22, 2013)
  We have been considering fits using:
  1) No compression
  2) logarithms with a constant offset ($log(x+10^-offset)$), where offset=1, 2, 3, 4, 5, or is fit via optimization
  3) nth-root filters, where n=1.5,2,2.5,3,4,5, or is fit via optimization

  Looking at preliminary results for 18 cells, we can make the following statements:
  1) Compression helps almost always.
  2) However, fitting the compressor via optimization failed to reach peak performance in every case and often prevented convergence. 
  3) For every good fitting nth-root compressor, there is always a better-fitting log compressor.
  4) For some strange reason, even numbered nth-root compressors (2,4,6) perform more poorly than odd ones (3,5,7, etc). This is weird. 
  5) The best log fit varies between log(x+10^-1) and log(x+10^-4). I presume these account for different spontaneous rates of fire.

  Hypotheses:
  1) Log compressors are beating the nth-root ones because they predict a nonzero firing rate even when there is no stimulus. 

  Future Experimental Options:
  1) Define a sqrt compressor with a nonzero firing rate for no stimulus. Phi = [zerorate, exponent]
     -> Decided it's a waste of time. Established scientific theory already says that we aren't really expecting this nonlinearity. 
  2) Define a logarithmic compressor which uses base 2, 3, 4, 5, 6 instead of e. Phi = [zerorate, exponent] 
     -> Decided not necessary, since FIR scaling can effectively transform one log base into another. They only differ by a constant factor. Remember: Log_a (x) / log_a(b) = log_b (x)
  3) Try a sigmoidal compressor to account for firing rate saturation. 
     -> TODO
* Fitting Routines (Jan 22, 2013)
  There is no "Best Fitting Routine", there are only fitting routines that work better in some circumstances than others.

  Five routines were tried: 
  1) fminsearch
  2) least squares,
  3) jackknifed lsq, 
  4) fminsearch with a smoothing weight, which has a penalty proportional to the RMS magnitude of the FIR filter coefs
  5) fminsearch followed by lsq

  Observations from testing on 18 cells:
  1) Lsqcurvefit() by itself fails to converge occasionally but is fast
  2) fminsearch() followed by lsqcurvefit() was always the best choice for cells with strong responses
  3) smoothed MSE fitter or Jackknifed fitter resulted in higher test scores for some cells with weaker responses (presumably because they avoided overfitting).
  4) Probably the best approach is to fit the model multiple times using different approaches, and take the one with the highest test score, and save it.

  Future Experiments
  1) Fit each element independently before doing the global final fit (could be done in auto-init)
  2) Bayesian likelihood fits
   
* Nonlinearities (Jan 22, 2013)
  Needs to be examined. 
  
* UPDATE (Jan 25, 2013)
  COMPRESSORS: Seems like Log1, log2, log3, log4 pretty much span the input range across 30 cells.
  FITTERS: fminlsq is the overall winner. Multi-step approaches did not help.
  LEVEL SHIFTING, LACK OF ATTENTION, OR MY EYE PLAYING TRICKS ON ME?: I swear to god that in many of these sound files, in stimulus #1, the very first edge (right after the sound begins) looks like it not neurally tracked like the others.
  FIR: Probably I should put fitting the FIR baseline back in, since it looks like it helps in many cases to clean up the filter visually.
** Why we can't pick the model with the best test set fit:
   Each fitting method will still have some 'noise' in the parameters
   Choosing a fitting method will choose one set of noise over another
   If we do ANY selection of model parameters based on TEST SET performance, we are picking a set of noise and that is a no-no
   However, we could of course pick the model with the best TRAINING SET performance, that would be fine.
  
