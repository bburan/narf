OBSERVATIONS ON VARIOUS MODELS

* General Model Structure.
  1. MODEL COMPRESSOR: Any nonlinearity before the FIR filter a "compressor". 
  2. FIR FILTER: Has sufficient coefficients to cover the past 100ms of history and span the input dimensions
  3. NONLINEARITY: A global nonlinearity on the output of the FIR filter. 

* Compressors (Jan 22, 2013)
  We have been considering fits using:
  1) No compression
  2) logarithms with a constant offset ($log(x+10^-offset)$), where offset=1, 2, 3, 4, 5, or is fit via optimization
  3) nth-root filters, where n=1.5,2,2.5,3,4,5, or is fit via optimization

  Looking at preliminary results for 18 cells, we can make the following statements:
  1) Compression helps almost always.
  2) However, fitting the compressor via optimization failed to reach peak performance in every case and often prevented convergence. 
  3) For every good fitting nth-root compressor, there is always a better-fitting log compressor.
  4) For some strange reason, even numbered nth-root compressors (2,4,6) perform more poorly than odd ones (3,5,7, etc). This is weird. 
  5) The best log fit varies between log(x+10^-1) and log(x+10^-4). I presume these account for different spontaneous rates of fire.

  Hypotheses:
  1) Log compressors are beating the nth-root ones because they predict a nonzero firing rate even when there is no stimulus. 

  Future Experimental Options:
  1) Define a sqrt compressor with a nonzero firing rate for no stimulus. Phi = [zerorate, exponent]
     -> Decided it's a waste of time. Established scientific theory already says that we aren't really expecting this nonlinearity. 
  2) Define a logarithmic compressor which uses base 2, 3, 4, 5, 6 instead of e. Phi = [zerorate, exponent] 
     -> Decided not necessary, since FIR scaling can effectively transform one log base into another. They only differ by a constant factor. Remember: Log_a (x) / log_a(b) = log_b (x)
  3) Try a sigmoidal compressor to account for firing rate saturation. 
     -> TODO
* Fitting Routines (Jan 22, 2013)
  There is no "Best Fitting Routine", there are only fitting routines that work better in some circumstances than others.

  Five routines were tried: 
  1) fminsearch
  2) least squares,
  3) jackknifed lsq, 
  4) fminsearch with a smoothing weight, which has a penalty proportional to the RMS magnitude of the FIR filter coefs
  5) fminsearch followed by lsq

  Observations from testing on 18 cells:
  1) Lsqcurvefit() by itself fails to converge occasionally but is fast
  2) fminsearch() followed by lsqcurvefit() was always the best choice for cells with strong responses
  3) smoothed MSE fitter or Jackknifed fitter resulted in higher test scores for some cells with weaker responses (presumably because they avoided overfitting).
  4) Probably the best approach is to fit the model multiple times using different approaches, and take the one with the highest test score, and save it.

  Future Experiments
  1) Fit each element independently before doing the global final fit (could be done in auto-init)
  2) Bayesian likelihood fits
   
* UPDATE (Jan 25, 2013)
  COMPRESSORS: Seems like Log1, log2, log3, log4 pretty much span the input range across 30 cells.
  FITTERS: fminlsq is the overall winner. Multi-step approaches did not help.
  LEVEL SHIFTING, LACK OF ATTENTION, OR MY EYE PLAYING TRICKS ON ME?: I swear to god that in many of these sound files, in stimulus #1, the very first edge (right after the sound begins) looks like it not neurally tracked like the others.
  FIR: Probably I should put fitting the FIR baseline back in, since it looks like it helps in many cases to clean up the filter visually.
** Why we can't pick the model with the best test set fit:
   Each fitting method will still have some 'noise' in the parameters
   Choosing a fitting method will choose one set of noise over another
   If we do ANY selection of model parameters based on TEST SET performance, we are picking a set of noise and that is a no-no
   However, we could of course pick the model with the best TRAINING SET performance, that would be fine.
   
* Feb 11, 2013: Analysis Batch 240 Notes
  "Best" values chosen with my eye for qualitative considerations like simplicity and smoothness. 
  Sort with m-x org-table-sort-lines if desired.

** STRF numbers have the following meaning:
    5=Excellent STRF. Consistent across nearly all trials, little energy outside expected 20-30ms band.
    4=Good. Clear signal, but noisier
    3=Fair. More energy in 0 coef suggesting something unmodeled. 
    2=Poor. Maybe a signal here somewhere?
    1=Crap. I can't see anything useful here.

** The "Saturation Level" column:
    low    (The NL and its derivative are increasing)
    linear (The NL is approximately linear) 
    high   (The NL is increasing but its derivative is negative)
    notch  (There is a point of inflection)

** DATA table
  | CELLID     | COMP   | NL       | BEST FITTER     | R^2 | STRF | NOTES                                             | Saturation level |
  |------------+--------+----------+-----------------+-----+------+---------------------------------------------------+------------------|
  | por025a-b1 | log2   | step     | fminlsq         | .55 |    5 |                                                   | Low              |
  | por025a-c1 | nocomp | npnl     | fminlsq/boost   | .34 |    5 | Depressing (.35)                                  | High             |
  | por025a-c2 | log2   | sig      | twostep         | .14 |    3 |                                                   | Low              |
  | por025a-d1 | log2   | sig/step | fminlsq         | .49 |    4 |                                                   | Low              |
  | por026a-b1 | log2   | step     | slsq            | .37 |  3.5 |                                                   | Low              |
  | por026a-d1 | log2   | npnl     | gene            | .16 |    2 | Lots energy at 30, 60-80ms!?                      | Notch            |
  | por026b-a1 | log2   | npnl     | sboost          | .57 |    5 |                                                   | Sigmoid          |
  | por026b-a2 | log2   | npnl     | fminlsq/twostep | .28 |    3 |                                                   | Linear           |
  | por026b-b1 | log2   | npnl     | boost           | .37 |    5 | Depressing (.39)                                  | Sigmoid          |
  | por026b-b2 | log2   | sig      | fminlsq         | .52 |    3 | Good R^2, but weird STRF.                         | Low              |
  | por026b-c1 | log2   | npnl     | sboost          | .20 |    5 |                                                   | Linear           |
  | por026b-c2 | nocomp | sig      | twostep         | .30 |    3 |                                                   | Low              |
  | por026b-d1 | nocomp | npnl     | boost           | .30 |    5 | Excellent STRF.                                   | Notch            |
  | por026c-a1 | log2   | step     | slsq            | .37 |    2 | Step clearly wrong choice.                        | Low              |
  | por026c-b1 | log2   | npnl     | fminlsq         | .34 |    3 | U-shaped NL.                                      | Notch            |
  | por026c-b2 |        |          |                 |     |    1 | Garbage                                           | Garbage          |
  | por026c-c1 | nocomp | npnl     | sboost          | .52 |    5 | Depressing (.53)                                  | Linear/High      |
  | por026c-d1 | nocomp | npnl     | boost           | .15 |    4 | Good if not overfit.                              | Notch            |
  | por026c-d2 | log2   | sig      | boost           | .15 |    3 | Good if not overfit.                              | Low              |
  | por027a-a1 | log2   | npnl     | boost           | .13 |    4 | Good if not overfit.                              | Sigmoid          |
  | por027a-b1 | log2   | sig      | ?               | .25 |    3 | Differential!?                                    | Linear           |
  | por027a-c1 | log2   | npnl     | boost           | .23 |    5 | Depressing (.23)                                  | High             |
  | por027b-b1 | log2   | npnl     | slsq            | .17 |    3 |                                                   | Low              |
  | por027b-c1 | nocomp | npnl     | ?               | .21 |    2 |                                                   | Sigmoid          |
  | por028b-b1 | log2   | npnl     | sgene           | .23 |    4 | U-shaped NL.                                      | Notch            |
  | por028b-c1 | log2   | npnl     | sboost          | .20 |    4 | Strange NL.                                       | Sigmoid          |
  | por028b-d1 | log2   | npnl     | slsq            | .29 |    5 | Depressing (.36) Good if not overfit.             | High             |
  | por028d-a1 | log2   | npnl     | fminlsq         | .25 |    2 | Strange NL.                                       | High             |
  | por028d-a2 | log2   | npnl     | sgene           | .18 |    3 |                                                   |                  |
  | por028d-b1 | log2   | npnl     | sboost          | .65 |    5 | Perfect. Boosts are cleaner, slsq more realistic. | Sigmoid          |
  | por028d-c1 |        |          |                 |     |      | Garbage                                           |                  |
  | por028d-c2 | log2   | step     | boost           | .12 |    2 | Plausible. Strong at 20, 70ms.                    | Low              |
  | por028d-d1 |        |          |                 |     |      |                                                   |                  |

** Observations
  - Nearly perfect: por028d-b1, por026b-a1
  - For "low" saturation levels, steps and sigs work better than NPNLs
    Neurons I thought were depressing last time may really just be saturating
    (por026b-b1, por026c-c1, por028b-d1, por025a-c1, por027a-c1, por028d-a1 )
  - Boosting avoids overfitting when R^2 < 0.3. For higher values, slsq or fminlsq wins.
  - Boosting gives 'sharp edges' on the response. Does this reflect reality?
  - U-shaped NL: por026c-b1, por028b-b1
  - Notched NL: por026b-d1, por026c-d1, por026a-d1  (All on same day?)
  - Late FIRs: por028d-c2, por026a-d1, (Probably due to depression)
  - Many times the sigmoid fits the upper or lower part, but not both. 
  - There is a tradeoff between FIR complexity and NL complexity
    por026c-c1 is a good example. 
    If log2 added, FIR is noisier but output is very linear. 
    If it's not there, FIR is cleaner but NL is curved.
    If there is simple NL (like a step or sigmoid), the FIR has more noise. 
    If there is a complex NL (like a NPNL), the FIR is simpler.
  - Annealing fitter is slow and not very good.
  - Genetic algorithm fitter is fast, but not very good.
  - Shrinkage fitters don't seem to help tremendously for non-boosting fitters
    Probably this is because non-boosting fitters are finding wildly different NLs.
    Wildly different NLs cannot be shrunk properly because they are nonlinear. 
  - Twostep fits more consistently than most algorithms.
    But, it never does as well as fitting the NL and FIR at the same time.
    
** Conclusions
   Stick with the NPNL, it is a good general case and is easy to understand.
   The best fitters are fminlsq, boost, slsq, and twostep.

** Going forward, what is needed?
   Log2 + Offset before the depression filter
   A 3-step fitter:
   A metric of FIR sparsity (L0 "norm": number of nonzero terms)
   A metric of FIR magnitude (L1 norm: Sum of abs values
   A metric of NL smoothness
   A check of homoskedasticity (How much is the variance changing for the nonlinearity along the abscissa)
   Playing around with the number of NPNL bins, extrapolation, and smoothness.
   Simulated annealing doesn't work well.
   Plot both test and training data for the NPNL.
   A stronger shrinkage filter
   A way to compare NPNLs for multiple data sets.
   A histogram heat map of model performance for each cell
   A Tikhonov matrix for regression: diagonals are variance of each coef. 2nd diagonals would add some correlation from one FIR coef to the next (smoothness?). 
   Automatic Relevancy Determination (ARD)
   Automatic Smoothness Determination (ASD)
   
  
