* NEW FEATURES
  1. [X] Try using a gaussian filter to get a NL
  2. [X] Try several SENL gaussian widths
  3. [X] Try several SENL extrapolation tricks
  4. [ ] Add a normalized FIR which always has output of same magnitude
  5. [ ] Sparse fitter which uses E = MSE(r_hat, r), + alpha*numel(h!=0)
  6. [ ] Try a NPNL with non-uniform bins and better extrapolation for ends
  7. [ ] Sparsity check:
	 For each model,
            for 1:num coefs
             Prune the least important coef
              plot performance
            Make a plot of the #coefs vs performance
  8. [ ] An MSE metric (and lsqcurvefit routine) or which makes single-sample bins and discards outliers 4 or 5 STDEVS from the NL curve
  9. [ ] CLEAN: sf=sf{1}; should be eliminated IN EVERY SINGLE FILE!
  10. [ ] CLEAN: 'unused' should become ~ IN EVERY FILE
  11. [ ] Add a log compressor as an extra channel
  12. [ ] FITTER: Zeroing/Relevancy boosting fitter. Uses scaling coefficient ALPHA. 0<ALPHA<10 is a good range. Higher values shrink harder, small values shrink less. 
	  After training with any algorithm, perturb all values by a few small random deltas. 
	  Compute the sample variance on the output and pretend that it's roughly the variance.
	  Shrink ONLY the 'least important' coefficient by multiplying it by (1/var)^alpha. 
	  That shrinkage is considered the 'step'.
	  Proceed until there is N% of the power. 
	  The solution should thus be sparse. Maybe not optimally so, but this is easy to implement.
  13. [ ] FITTER: Boost/relevancy fitter. Does relevancy pruning as it boosts. Convergence is harder to achieve in this case, however. 
  14. [ ] CLEAN: Fix fit_sparsebayes() so that it won't ever have positive definite problems 
	  positivedefinite = all(eig(basis) > 0);
	  OR use the 2nd return value of chol() when running the decomposition in sparse bayes
  15. [ ] CLEAN: Compare_models needs to sort based on training score if test_score doesn't exist.
  16. [ ] FITTER: Three-step fitter (First FIR, then NL, then both together)
  17. [ ] A metric of FIR sparsity (L0 "norm": number of nonzero terms)
  18. [ ] A metric of FIR magnitude (L1 norm: Sum of abs values)
  19. [ ] A metric of NL smoothness (L1 or L2 norm of derivative of filter)
  20. [ ] Expressing NL smoothness regularizer as a matrix
	  A Tikhonov matrix for regression: 
	  diagonals are variance of each coef.
	  2nd diagonals would add some correlation from one FIR coef to the next (smoothness?).
  21. [ ] A check of NL homoskedasticity (How much is the variance changing along the abscissa)
  22. [ ] FITTER: Log Likelihood because MSE is biased towards gaussian noise models, and for real-life data sets the probability tails are always heavier than a gaussian. 
  23. [ ] Plot both test and training data for the NPNL and SENL
  24. [ ] FITTER: A stronger shrinkage fitter (Shrink by as much as you want)
  25. [ ] A way to compare NPNLs for multiple data sets.
  26. [ ] A histogram heat map of model performance for each cell
  27. [ ] Automatic Relevancy Determination (ARD)
  28. [ ] Automatic Smoothness Determination (ASD)
  29. [ ] Get a histogram of the error of the NL. (Is it Gaussian or something else?)
  30. [ ] FITTER: Fit the NL in a way that DISCARDS outliers. 
  31. [ ] np2 = 'Nonparametric nonlinearity v2' (nbins, smoothing, extrapolation)
  32. [ ] Generalize N-step fitter to let a particular module or set of modules float. 
  33. [ ] PERF METRIC: inter_spike_intervals + bayesian_likelihood()
  34. [ ] I think normalizing the FIR filter coefs BEFORE doing the mean across jackknifes would really help. 
	  Probably output nonlinearities need to be refit after any averaging, as well.
  35. [ ] FN: Plot multiple models' NPNLs on the SAME PLOT?
  36. [ ] FN: Cleanliness of a FIR filter based on weight of DC coef, and how 'peaky' it is. Set highest peak to zero, then compare overall magnitude to before. This gives a series of moments.
  37. [ ] FN: 'set_module_field' (finds module, sets field, so you can mess with things later in scripts)
  38. [ ] MODULE: Make a faster IIR filter with asymmetric response properties 
  39. [ ] FITTER: SWARM. Hybrid fit routine which takes the top N% of models, scales all FIR powers to be the same, then shrinks them.
  40. [ ] MODULE: Make Concat Second Order Terms work for any higher order nchoosek type stuff
  41. [ ] MODULE: Standardized single/multi channel gammatone filter
  42. [ ] MODULE: Standardized single/multi channel elliptic filter 
  43. [ ] MODULE FN: Provide functions to cover the input space logarithmically with filters
  44. [ ] MODULE INIT: Make a module which has a complex init process
	  1) Creates a spanning filterbank of gammatones
	  2) Trains the FIR filter on that spanning filterbank
	  3) Picks the top N (Usually 1, 2 or 3) filters based on their power
	  4) Crops all other filters
  45. [ ] MODULE FN: Provide an auto-init for the filters which cover the input space, train filters on that, and picks the channel with the most power. It does this once wide, then once narrow.
  46. [ ] MODULE: Add a module which can pick out a particular dimension from a vector and give it a name as a signal
  47. [ ] REFACTOR: Replace all the 'true' and 'false' arguments with textual flags and varargin that are more descriptive
  48. [ ] REFACTOR: the Fitters because they are all pretty much the same damn thing over and over again
  49. [ ] MODULE: Build a non-cheating model which extracts envelopes directly from the WAV files using an elliptic or gammatone prefilter
  50. [ ] FN: Cleaning function which DELETES any models which have NaNs for test/train fits
  51. [ ] FITTER: Import fitting routines from STRFlab
  52. [ ] Roll model summary caches and select_summaries into Stephen's BAPHY, since in the end all I did was reinvent yet another crappy RDBMS

* END USER CONVENIENCES
  1. [ ] Why isn't auto recalc the default?
  2. [ ] Make logging work for the GUI by including the log space in narf_modelpane?
  3. [ ] IRRITATION: Why doesn't 'nonlinearity' module default to a sigmoid with reasonable parameters?
  4. [ ] IRRITATION: Why doesn't it show the model save filename so I can see which file I just loaded if I forgot?
  5. [ ] IRRITATION: Why doesn't every plotted signal have a legend?
  6. [ ] IRRITATION: Why don't the X (or at least the Y) axes have scales?
  7. [ ] IRRITATION: Why isn't there an 'undo' function?
  8. [ ] IRRITATION: Why can't I resize windows?
  9. [ ] IRRITATION: Why isn't there progress in the GUI when fitting?
  10. [ ] IRRITATION: Why are the editable text boxes so damn small?
  11. [ ] IRRITATION: Why can't I edit a module type in the middle of the stack via the GUI?
  12. [ ] Write a crash course guide on using NARF
  13. [ ] Remember to invalidate data BELOW the present point on a table-edit callback... and to update the gui to reflect this!

* BUG FIXES AND CLEANING
  1) [X] Paths have become a bit messy: grep for NARF_PATH and correct (also: replace with filesep when possible)
  2) [X] Cleaner way of building models in a script than accessing by index number?
  3) [X] Look for obvious repetition and make some more functions in util/
  4) [ ] Many repeated blocks of code have evolved and need to be destroyed.
  5) [ ] Names probably could use some rethinking as well, especially defaults (like using 'stim' default even in the fitting algorithms, for example)
  6) [ ] Add error handling (catch/throw) around EVERY CALL to a user defined function
  7) [ ] In retrospect, 'gui' and 'plot_gui' stuff probably shouldn't be stored in the XXX or STACK structures...should it be in a 3rd structure?
  8) [ ] Ensure that no closures of data are being done by methods. Methods should accept the module object as their first argument, not close over anything.
  9) [ ] It's not quite right to have the 'replot' command be part of the the 'plot_popup fn callback'. Needs to be re-thought.
  10) [ ] Go through the TODO's, FIXME's, etc in existing files
  11) [ ] Create a module methods directory for shared methods
  12) [ ] Create a module keywords directory for helping with combinatoric name management.
  13) [ ] make anything named 'do_' into a method for use with modules?
  14) [ ] make anything named 'update_' into a function used purely for its side effects?
  15) [ ] Delete the GUI objects whenever you 'apply' since they may need to be recreated?
	  
* DESIGN QUESTIONS TO BRAINSTORM:
  1. [X] How can sane initial conditions for optimization be automatically arrived at without extra script-writing?
	 Auto-initialization of model params is done by allowing modules to update their design based on the data by calling the optional 'auto_init' method.
	 Arg 1 is the STACK, not including the model itself. 
	 Arg 2 is the XXX data input, not including the model's output data itself. 
  2. [X] How can jack-knifing be integrated in to the optimization routine to prevent over-fitting?
	 Split the big long RESP and STIM vectors in fit_with_lsqcurvefit into 10 chunks
	 Take groups of 9 of those chunks, run lsqcurvefit, then test on remaining chunk
	 Take weighted average of all jackknifed solutions, weighting each by inverse variance? Or just mean, if we assume they all have same variance?
	 Return weighted average.
  3. [X] How should optimization constraints be incorporated in the design?
	 Probably the easiest way is to define a structure which may be used by pack/unpack to create upper and lower bounds, which are then passed to the optimization routine
	 opt_hints = struct('alpha', [-1 3], 'beta', [0 inf]); % Constrain alpha from -1 to 3 and beta from 0 to infinity. 
  4. [X] How should models be automatically generated in a quick and scriptable way?
	 See analysis/test_likely_candidates.m
  5. [X] How can design internal degrees of freedom be detected and corrected during optimization?
	 (Probably they cannot!)
  6. [X] There needs to be a place to store information about a whole model. 
	 For example, 'model name' and 'fitter' are two examples of fields that don't really belong in a module.
  7. [X] There is no best fitting routine, only fitting routines which work better for different cells. Allow them all a chance to run by making them module parameters.
  8. [X] Can jackknifing or the equivalent be applied to ANY fitting routine as a higher level function
	 If we only have one data file, how can we hold out some fraction of the stimuli so that we can do training/test on a single data file?
	 Solution:
	 - Fit routines use a 'score'
	 - The stack gives the score
	 - The score needs to be calculated from a jackknife
	 - How can data be jackknifed without modifying the stack?
	 - Immediately after the loading, zero a chunk of the stim and respavg (save the original, of course)
	 - Do a fit with whatever routine you want
  9. [X] N-step fitter (train FIR in common, train NL across each separately)
	 Surprisingly difficult to make several models need to be fit all on the same data. yet ALSO need to run on different behavioral states. 
         1. Violates my implicit expectation of 1 fitter -> 1 model. Now I have 1 fitter-> many models.
	 2. Now that training_set{} may be edited, it shouldn't really be copied from one XXX{1} to XXX{2} and so on.
	 Solution ideas: 
	 - Quick hack: five new fitters added
	   NL1, trains on all, but only trains NL on 1st
	   NL2, trains FIR on all, but only trains NL on 
  10. [ ] Right now, it's very convenient to be able to have the 'fitter' and 'score' quantity to be in modules
	  I can plug in all the module groups and let the fitter run. I can compare different fit routines automatically.
	  However, a fitter is not really part of a module, it's part of a whole model.
	  Therefore, in the future, the fitter and score quantity should be stored in the model META structure.
	  On the other hand, I need to justify this: Why should this be done instead of leaving it in the STACK? What we have right now works and is convenient.
	  (Because we may want to try multiple fit routines, and pick the model with the best training score?)
	  (Because I expect that model specific fitters are necessary? That isn't a reason!)
  11. [ ] Right now, you can only instantiate a single GUI at a time. Could this be avoided and the design made more general?	  
	  To do this, instead of a _global_ STACK and XXX, they would be closed-over by the GUI object.
	  Then, there would need to be a 'update-gui' function which can use those closed over variables.
	  That fn could be called whenever you want to programmatically update it. 	  	  	 
  12. [ ] It is awkward in non-parametric non-linearity module to recalc the phi every time you need it for graphing. Some place to cache it would be good without risking cache staleness.
  13. [X] Nonparametric Nonlinearity (NPNL) linearizes anything. 
	  It is very much data-driven, which is great. 
	  On the other hand, it fits itself to linearize almost anything, so we somehow learn less than a simple, parameter-driven model. 
	  How can we balance complexity in the FIR or complexity in the NL?
	  ANSWER: Sparseness needs to be modeled on the FIR side, Smoothness on the NL side. 
  14. [ ] Are neurons clusterable according to which models describe them well?
	  Are they really different populations of neurons, or just points along a continuum?

  15. [ ] ENDGAME: 
	  Is the end goal of this system something that:
	  - Spans the input space of nonlinearities?
	  - Spans the input space of depression?
	  - Has an inhibition and excitation filter?
	  - Has a NPNL for inhibition, and a NPNL for excitation?
	  - Uses ARD to eliminate all unimportant dimensions?
	  - Reports the best model?

* LUXURY, UNESSENTIAL TODO ITEMS 
  - [ ] Make it so baphy can be run _twice_, so that raw_stim_fs can be two different values (load envelope and wav data simultaneously)
  - [ ] Make gui plot functions response have two dropdowns to pick out colorbar thresholds for easier visualization?
  - [ ] MODULE: Add a filter that processess phase information from a stimulus, not just the magnitude
  - [ ] Write a function which swaps out the STACK into the BACKGROUND so you can 'hold' a model as a reference and play around with other settings, and see the results graphically by switching back and forth.
  - [ ] Write dbchoosecellfiles() and connect it to NARF_MODELPANE
  - [ ] Try adding informative color to histograms and scatter plots
  - [ ] Try improving contrast of various intensity plots
  - [ ] Put a Button on the performance metric that launches an external figure if more plot space is needed.
  - [ ] Add a GUI button to load_stim_from_baphy to play the stimulus as a sound
  - [ ] FITTER: Crop N% out fitter:
	  1) quickfits FIR
	  2) then quickfits NL, 
	  3) measures distance from NL line, marks the N worst points
	  4) Looks them up by original indexes (before the sort and row averaging)
	  5) Inverts nonlinearity numerically to find input
	  6) Deconvolves FIR to find the spike that was bad
	  7) Deletes that bad spike from the data
	  8) Starts again with a shrinkage fitter that fits both together
	     
