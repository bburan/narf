* ROADMAP
  1. [X] Get NARF running on 4 machines
  2. [ ] Before running sb, clear the params so you start from zero?
  3. [ ] Choose a 'best on average' amount of sparseness penalty
  4. [ ] If empty test set is given for a cellid, what should we do? Hold 1 out cross validation?
  5. [ ] Make a new summary for cellids, with HTML data table output with integrated images instead of 5 different plots.
  6. [ ] Input-spanning depression bank. 
  7. [ ] Start leveraging Stephen's job distribution code
  8. [ ] Cleaning, bugfixes, and user interface changes that have accumulated and are burying me
  9. [ ] Start using BAPHY for caching analysis results

* WORK
** MINOR ADDITIONS
  1. [ ] Add a keyword with a log compressor as an extra channel
  2. [ ] Plot both test and training data for smooth scatter plots.
  3. [ ] Standardize colors for plot lines. 
  4. [ ] A histogram heat map of model performance for each cell so you can see distribution of model performance
  5. [ ] A way to compare nonlinearities for multiple data sets.  
  6. [ ] FN: 'set_module_field' (finds module, sets field, so you can mess with things later in scripts)

** CLEANING AND BUGFIXES:
  1. [ ] Ooops, the git commit is not really being saved! It should go in META, not just in the cache
  2. [ ] CLEAN: Compare_models needs to sort based on training score if test_score doesn't exist.
  3. [ ] CLEAN: sf=sf{1}; should be eliminated IN EVERY SINGLE FILE!
  4. [ ] CLEAN: 'unused' should become ~ IN EVERY FILE
  5. [ ] CLEAN: Fix fit_sparsebayes() so that it won't ever have positive definite problems. eg, positivedefinite = all(eig(basis) > 0) OR use the 2nd return value of chol() when running the decomposition in sparse bayes
  6. [ ] REFACTOR: Replace all the 'true' and 'false' arguments with textual flags and varargin that are more descriptive
  7. [ ] REFACTOR: the Fitters because they are all pretty much the same damn thing over and over again
  8. [ ] Grep for TODO's, FIXME's, etc in existing files
  9. [ ] Search for obviously dead code and bury it in the graveyard
  10. [ ] Search for repeated blocks of code and refactor
  11. [ ] Put proper docstrings on every function in util/
  12. [ ] Check paths again, grep for NARF_PATH and correct (also: replace / with filesep when possible)
  13. [ ] Names probably could use some rethinking as well, especially defaults (like using 'stim' default even in the fitting algorithms, for example)
  14. [ ] Add error handling (catch/throw) around EVERY CALL to a user defined function
  15. [ ] In retrospect, 'gui' and 'plot_gui' stuff probably shouldn't be stored in the XXX or STACK structures...move it be in a 3rd structure?
  16. [ ] Ensure that no closures of data are being done by methods. Methods should accept the module object as their first argument, not close over anything.
  17. [ ] It's not quite right to have the 'replot' command be part of the the 'plot_popup fn callback'. Needs to be re-thought.
  18. [ ] Create a module methods directory for shared methods
  19. [ ] Create a module keywords directory for helping with combinatoric name management instead of 'module groups'? 
  20. [ ] make anything named 'do_' into a method for use with modules?
  21. [ ] make anything named 'update_' into a function used purely for its side effects?
  22. [ ] Delete the GUI objects whenever you 'apply' since they may need to be recreated?

** FITTERS:
  1. [ ] FITTER: A lsqcurvefit with sparseness penalty (somehow read from later on?)
  2. [ ] FITTER: A stronger shrinkage fitter (Shrink by as much as you want)
  3. [ ] FITTER: Log Likelihood because MSE is biased towards gaussian noise models, and for real-life data sets the probability tails are always heavier than a gaussian. 
  4. [ ] FITTER: Three-step fitter (First FIR, then NL, then both together)

** LARGER WORK
  1. [ ] Generalize N-step fitter to let a particular module or set of modules float. 
  2. [ ] inter_spike_intervals 
  3. [ ] bayesian_likelihood() perf metric
  4. [ ] Use a single wavelet transform in place of downsampling + FIR filter (Hard and slow to fit, but extremely general)
  5. [ ] Automatic Relevancy Determination (ARD)
  6. [ ] Automatic Smoothness Determination (ASD)
  7. [ ] Roll model summary caches and select_summaries into Stephen's BAPHY, since in the end all I did was reinvent yet another crappy RDBMS

** MODULES:
  1. [ ] MODULE FN: Provide functions to cover the input space logarithmically with filters
  2. [ ] MODULE: Make a faster IIR filter with asymmetric response properties 
  3. [ ] MODULE: Make Concat Second Order Terms work for any higher order nchoosek type stuff
  4. [ ] MODULE: Standardized single/multi channel gammatone filter
  5. [ ] MODULE: Standardized single/multi channel elliptic filter 
  6. [ ] MODULE INIT: Make a module which has a complex init process
	 1) Creates a spanning filterbank of gammatones
	 2) Trains the FIR filter on that spanning filterbank
	 3) Picks the top N (Usually 1, 2 or 3) filters based on their power
	 4) Crops all other filters
  7. [ ] MODULE FN: Provide an auto-init for the filters which cover the input space, train filters on that, and picks the channel with the most power. It does this once wide, then once narrow.
  8. [ ] MODULE: Add a module which can pick out a particular dimension from a vector and give it a name as a signal
  9. [ ] MODULE: Build a non-cheating model which extracts envelopes directly from the WAV files using an elliptic or gammatone prefilter

** END USER CONVENIENCES
  1. [ ] Why isn't auto recalc the default?
  2. [ ] Make logging work for the GUI by including the log space in narf_modelpane?
  3. [ ] IRRITATION: Why doesn't 'nonlinearity' module default to a sigmoid with reasonable parameters?
  4. [ ] IRRITATION: Why doesn't it show the model save filename so I can see which file I just loaded if I forgot?
  5. [ ] IRRITATION: Why doesn't every plotted signal have a legend?
  6. [ ] IRRITATION: Why don't the X (or at least the Y) axes have scales?
  7. [ ] IRRITATION: Why isn't there an 'undo' function?
  8. [ ] IRRITATION: Why can't I resize windows?
  9. [ ] IRRITATION: Why isn't there progress in the GUI when fitting?
  10. [ ] IRRITATION: Why are the editable text boxes so damn small?
  11. [ ] IRRITATION: Why can't I edit a module type in the middle of the stack via the GUI?
  12. [ ] Write a crash course guide on using NARF
  13. [ ] Remember to invalidate data BELOW the present point on a table-edit callback... and to update the gui to reflect this!
  
* DESIGN QUESTIONS TO BRAINSTORM:
  1. [X] How can sane initial conditions for optimization be automatically arrived at without extra script-writing?
	 Auto-initialization of model params is done by allowing modules to update their design based on the data by calling the optional 'auto_init' method.
	 Arg 1 is the STACK, not including the model itself. 
	 Arg 2 is the XXX data input, not including the model's output data itself. 
  2. [X] How can jack-knifing be integrated in to the optimization routine to prevent over-fitting?
	 Split the big long RESP and STIM vectors in fit_with_lsqcurvefit into 10 chunks
	 Take groups of 9 of those chunks, run lsqcurvefit, then test on remaining chunk
	 Take weighted average of all jackknifed solutions, weighting each by inverse variance? Or just mean, if we assume they all have same variance?
	 Return weighted average.
  3. [X] How should optimization constraints be incorporated in the design?
	 Probably the easiest way is to define a structure which may be used by pack/unpack to create upper and lower bounds, which are then passed to the optimization routine
	 opt_hints = struct('alpha', [-1 3], 'beta', [0 inf]); % Constrain alpha from -1 to 3 and beta from 0 to infinity. 
  4. [X] How should models be automatically generated in a quick and scriptable way?
	 See analysis/test_likely_candidates.m
  5. [X] How can design internal degrees of freedom be detected and corrected during optimization?
	 (Probably they cannot!)
  6. [X] There needs to be a place to store information about a whole model. 
	 For example, 'model name' and 'fitter' are two examples of fields that don't really belong in a module.
  7. [X] There is no best fitting routine, only fitting routines which work better for different cells. Allow them all a chance to run by making them module parameters.
  8. [X] Can jackknifing or the equivalent be applied to ANY fitting routine as a higher level function
	 If we only have one data file, how can we hold out some fraction of the stimuli so that we can do training/test on a single data file?
	 Solution:
	 - Fit routines use a 'score'
	 - The stack gives the score
	 - The score needs to be calculated from a jackknife
	 - How can data be jackknifed without modifying the stack?
	 - Immediately after the loading, zero a chunk of the stim and respavg (save the original, of course)
	 - Do a fit with whatever routine you want
  9. [X] N-step fitter (train FIR in common, train NL across each separately)
	 Surprisingly difficult to make several models need to be fit all on the same data. yet ALSO need to run on different behavioral states. 
         1. Violates my implicit expectation of 1 fitter -> 1 model. Now I have 1 fitter-> many models.
	 2. Now that training_set{} may be edited, it shouldn't really be copied from one XXX{1} to XXX{2} and so on.
	 Solution ideas: 
	 - Quick hack: five new fitters added
	   NL1, trains on all, but only trains NL on 1st
	   NL2, trains FIR on all, but only trains NL on 
  10. [ ] Right now, it's very convenient to be able to have the 'fitter' and 'score' quantity to be in modules
	  I can plug in all the module groups and let the fitter run. I can compare different fit routines automatically.
	  However, a fitter is not really part of a module, it's part of a whole model.
	  Therefore, in the future, the fitter and score quantity should be stored in the model META structure.
	  On the other hand, I need to justify this: Why should this be done instead of leaving it in the STACK? What we have right now works and is convenient.
	  (Because we may want to try multiple fit routines, and pick the model with the best training score?)
	  (Because I expect that model specific fitters are necessary? That isn't a reason!)
	  ANSWER: A better way to achieve this type of thing would be to have mutating functions which mutate a default copy of the stack. (Kind of like how calculus of variations work)
	  By picking and choosing and intercombining these mutating functions, you could come up with many different variations.
	  They also would not be restricted to the somewhat arbitrary groupings which I came up with, and would let multiple parts of stack be mutated simultaneously.
  11. [ ] Right now, you can only instantiate a single GUI at a time. Could this be avoided and the design made more general?	  
	  To do this, instead of a _global_ STACK and XXX, they would be closed-over by the GUI object.
	  Then, there would need to be a 'update-gui' function which can use those closed over variables.
	  That fn could be called whenever you want to programmatically update it. 	  	  	 
  12. [ ] It is awkward in non-parametric non-linearity module to recalc the phi every time you need it for graphing. Some place to cache it would be good without risking cache staleness.
  13. [X] Nonparametric Nonlinearity (NPNL) linearizes anything. 
	  It is very much data-driven, which is great. 
	  On the other hand, it fits itself to linearize almost anything, so we somehow learn less than a simple, parameter-driven model. 
	  How can we balance complexity in the FIR or complexity in the NL?
	  ANSWER: Sparseness needs to be modeled on the FIR side, Smoothness on the NL side. 
  14. [X] How can LSQ curve fit use sparseness and smoothness metrics?
	  You can cheat and destroy the module system by looking later in the STACK for the MSE element. 
	  If the MSE module exists and has nonzero weights, add a bogus zero element the LSQ target vector, and a bogus LSQ prediction vector element with a value of the sqrt(smoothness_penalty).	  
	  
  15. [ ] Are neurons clusterable according to which models describe them well?
	  Are they really different populations of neurons, or just points along a continuum?
  16. [ ] ENDGAME: 
	  Is the end goal of this system something that:
	  - Spans the input space of nonlinearities?
	  - Spans the input space of depression?
	  - Has an inhibition and excitation filter?
	  - Has a NPNL for inhibition, and a NPNL for excitation?
	  - Uses ARD to eliminate all unimportant dimensions?
	  - Reports the best model?

*  UNESSENTIAL TODO ITEMS
  - [ ] Make gui plot functions response have two dropdowns to pick out colorbar thresholds for easier visualization?
  - [ ] Make it so baphy can be run _twice_, so that raw_stim_fs can be two different values (load envelope and wav data simultaneously)
  - [ ] MODULE: Add a filter that processess phase information from a stimulus, not just the magnitude
  - [ ] Write a function which swaps out the STACK into the BACKGROUND so you can 'hold' a model as a reference and play around with other settings, and see the results graphically by switching back and forth.
  - [ ] Try adding informative color to histograms and scatter plots
  - [ ] Try improving contrast of various intensity plots
  - [ ] Put a Button on the performance metric that launches an external figure if more plot space is needed.
  - [ ] Add a GUI button to load_stim_from_baphy to play the stimulus as a sound
  - [ ] FITTER: Crop N% out fitter:
	  1) quickfits FIR
	  2) then quickfits NL, 
	  3) measures distance from NL line, marks the N worst points
	  4) Looks them up by original indexes (before the sort and row averaging)
	  5) Inverts nonlinearity numerically to find input
	  6) Deconvolves FIR to find the spike that was bad
	  7) Deletes that bad spike from the data
	  8) Starts again with a shrinkage fitter that fits both together
  - [ ] Expressing NL smoothness regularizer as a matrix
	  A Tikhonov matrix for regression: 
	  diagonals are variance of each coef.
	  2nd diagonals would add some correlation from one FIR coef to the next (smoothness?).
  - [ ] Sparsity check:
	 For each model,
            for 1:num coefs
             Prune the least important coef
              plot performance
            Make a plot of the #coefs vs performance
  - [ ] A check of NL homoskedasticity (How much is the variance changing along the abscissa)	     
  - [ ] FITTER: SWARM. Hybrid fit routine which takes the top N% of models, scales all FIR powers to be the same, then shrinks them.
  - [ ] Get a histogram of the error of the NL. (Is it Gaussian or something else?)
