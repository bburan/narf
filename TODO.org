* NEXT STEPS:
  1. [X] Train a model on five data sets, then let a parameter be free for each data set and train again. 
	 Record and display the changes for each model trained on each data set.
  2. [X] FIX THIS PROBLEM:
	 Right now XXX{1} holds the initialization data which selects the training, test sets.
	 This is model-independent; XXX is clearly separate from the model STACK
	 However, saving the model STACK doesn't save XXX! So you lose track of which sets it was trained on when you save. 
	 Loading the stack later doesn't necessarily help. 
	 The solution I am going to use is to save the XXX{1} struct at the same time, and load it at the same time. 
	 If you want, you can put extra experimental meta-data in the XXX{1} struct and it will be saved as well. 
	 This maintains the partition between model and data.
  3. [X] Correct correlation module so that it works with NaN's.
  4. [ ] Try nlinfit 
  5. [ ] fminval investigation to use an arbitrary evaluation function
	 Train stuff using correlation (should give different fits than MSE)
  6. [ ] Make a function which extracts desired information from SAVED FILES
	 EX: Extract coefs
	 EX: extract scores	  
  7. [ ] Think about how to guess initial conditions for fitter in a extendable way
	 (I think the answer is simply to do it manually in each model-specific fitting function)
  8. [ ] Brainstorm how to 'autogenerate' models
  9. [ ] Build another analysis which works directly from the WAV files, and does a prefilter
  10. [ ] Generalize fitting constraints to work for any module
	 % Constrain phi to be >=0 , but allow coefs to be anywhere
	 % TODO: This type of crap needs to be generalized to allow multiple fit
	 % fields for the same module. Right now it assumes fit_fields is a
	 % length-one cell array
	 % LBcoefs = -inf * ones(1, numel(STACK{4}.(char(STACK{4}.fit_fields))));
	 % LBphi   = zeros(1,  numel(STACK{5}.(char(STACK{5}.fit_fields)))); 
	 % LB = [LBcoefs LBphi]';
	 % UB = inf*ones(numel(LB),1);

* SCIENTIFIC QUESTIONS
  1. Should training set data be weighted by manually-set stimfile weights to account for biasing effect of sheer large numbers of samples?
  2. By how much does training on more than one data set improve test performance?
  3. Should training occur all at once or in multiple passes?
     - Datapoint: Training the linear filter, then the exponential, then both at same time. Yields the same result as just training both in a single pass.
     - Datapoint: Training the linear filter, then the exponential. Yields an inferior result.   

* LATER TODO LIST
  1. [ ] Make inter_spike_intervals work
  2. [ ] Make bayesian_likelihood() work
  3. [ ] Make Concat Second Order Terms work for any higher order nchoosek type stuff
  5. [ ] Get linear_fit_with_preprocessing working again (working from a WAV file, that is)
  6. [ ] Make gammatone filter bank work like elliptic bandpass filter so interface is standard
  7. [ ] Provide functions to cover the input space logarithmically with filters
  8. [ ] Add a module which can pick out a particular dimension from a vector and give it a name as a signal
  9. [ ] SMOOTHING of the RESPAVG signal with gaussian convolution (optionally. But this corrupts data)
  10. [ ] Write a crash course guide on using NARF
  11. [ ] Remember to invalidate data BELOW the present point on a table-edit callback... and to update the gui to reflect this!
  12. [ ] Make logging work for the GUI by including the log space in narf_modelpane? 
  13. [ ] Delete the GUI objects whenever you 'apply' since they may need to be recreated?
  14. [ ] Write a 'scaled boosting' algorithm, which takes a step in the direction inversely proportional to power of that channel (reweighting the channels by their power, essentially)
  15. [ ] Write a 'conjugate boosting' algorithm, which is normal boosting but takes steps in a single direction until the objective function stops improving.
  16. [ ] Write several different performance metric functions: MSE and a point-process fit
  17. [ ] Write a jack-knifing optimization which can work with any of the optimization modules 
	  Basically, it should go through and ONLY take certain stimuli # for testing or training
  18. [ ] Single channel gammatone filter (for speed, once sensitivity has been identified)
  19. [ ] An option for taking correlation in a different method than sheer concatenation could be interesting
  20. [ ] Add a method to "save analysis", connecting the stimfile train/test sets, model structure and params, optimization method, and GIT code hash number all together in a single, savable structure.
  21. [ ] Add error handling (catch/throw) around EVERY CALL to a user defined function
  22. [ ] Handle NaN's better...right now they can cause problems! (Use nanmean())
  23. [ ] Add a GUI button to load_stim_from_baphy to play the stimulus as a sound?
  24. [ ] Put a Button on the performance metric that launches an external figure if more plot space is needed.
  25. [ ] Make it so baphy can be run _twice_, so that raw_stim_fs can be two different values (load envelope and wav data simultaneously)
  26. [ ] Suggest an improvement: Use BAPHY to cache intermediate values
	  
* CLEANING/REFACTORING TODOS:
  - Use this idiom more often to search through struct or cell arrays:
    hits = arrayfun(@(x)strcmp(x.stimfile, sf), XXX{2}.cfd);   % Use cellfun instead of arrayfun if needed.
  - Look for obvious repetition and make some more functions in util/
  - Remove/rename useless functions in util that have accumulated.
  - Rename things to be more clear. Any sort of input dimension is a 'channel', to abstract the notion of stimulus dimension?
  - In retrospect, 'plot_gui' stuff probably shouldn't be stored in the XXX or STACK structures...should it be in a 3rd structure?
  - It's not quite right to have the 'replot' command be part of the the 'plot_popup fn callback'. Needs to be re-thought.
  - Right now, you can only instantiate a single GUI at a time. Could this be avoided and the design made more general?
  - Go through the TODO's in existing files
  - Ensure that no closures of data are being done by methods. Methods should accept the module object as their first argument, not close over anything.
  - make anything named 'update_' into a function used purely for its side effects
  - make anything named 'do_' into a method for use with modules?

* LUXURY, UNESSENTIAL TODO ITEMS 
  - [ ] Make raw/stimulus response have two dropdowns to pick out colorbar thresholds for easier visualization
  - [ ] Add a filter that processess phase information from a stimulus, not just the magnitude
  - [ ] Write a function which swaps out the STACK into the BACKGROUND so you can 'hold' a model as a reference and play around with other settings, and see the results graphically by switching back and forth.
  - [ ] Write dbchoosecellfiles()
  - [ ] Use inter_curve_v3 to interactively make FIR things
  - [ ] Try adding color to histograms and scatter plots
  - [ ] Try improving contrast of various intensity plots
  - [ ] Add BIC or AIC to model comparison data
  - [ ] Optimization report card and status information logged
  - [ ] Rank model fits and plot correlations
  - [ ] New training method:
	1. Search for coefficients from a spanning filter bank
        2. Find the signal contributing most (using the FIR coefficients)
	3. Do a second filter bank that is only an octave wide, with the signal in the center, to get more substructure
