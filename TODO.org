* Design of modular model components will rely on closures and zero-argument querying (No need to add a full protocol structure yet!)
  1. At startup, directory is read, and preproc_filter_popup is initialized
     - [X] How is that converted into prettier strings? (By calling the filter with no arguments, the default arguments are returned)
     - [ ] What happens if one filter has an error during eval? (It will be logged and dropped)
  2. The preproc_data_table needs to be initialized with parameters.
     - [X] Where does the params come from? (By calling the filter with no arguments)
     - [X] Where is the data stored in the global GS? (GS.preproc_params, GS.downsampling_params, GS.model_params, GS.stochastic_params)
  3. Some clicks are made on the preproc window
     - [ ] the GS.preproc_params struct and preproc_params_mask is updated each time
  4. Somebody clicks save
     - [ ] Where is it saved? (Now: to a saved file via file dialog box. Later: To Baphy)
     - [ ] What is the filename? (User-determined)
  5. Somebody clicks load
     - [ ] Which file is loaded (Now: from a saved file via dialog box. Later: from Baphy)
  6. Eventually, somebody clicks the "preprocess" button  (OR A SCRIPT STARTS)
     - [ ] A closed-over function is created, which all an automatic script would care about anyway.
     - [ ] Data is verified
     - [ ] raw_data is filtered 
     - [ ] The plots are updated (If it's the first time being called, not all GS globals will be ready.)
  7. At a later point, somebody clicks "optimize"
     - [ ] Using the GS.preproc_params_mask and GS.preproc_params, a vector is made and handed to the optimization thing. 
     - [ ] During optimization, from the vector, GS.preproc_params_mask and GS.preproc_params, a 'preproc_params' structure is built
     - [ ] As usual, this is turned into a closed-over function
     - [ ] The filtering occurs, then other model steps.
     - [ ] Performance is measured, and you go back to the beginning of this step a bunch of times. 
     - [ ] At the end of the optimization, the preproc filter window is updated to reflect the new data. 
* Complexities:
  We may need to do an iteration procedure that treats one part of the model (IE, Linear FIR filters) differently from a nonlinear part (In my opinion, this is just a special case sampler)
  When a function is called with no arguments, it returns a user-visible string and a default parameters struct.
  Inner functions must return their parameters struct when given no arguments. 
  If you modify a function after starting up narf_gui, what will happen? (Right now, changes to the pretty-name and params will not be altered without restarting narf_gui, however if you fix the function itself then that is fine.)

* LEFT OFF THINKING ABOUT THIS
  1. Try Loading a struct from an mfile,
  2. Try displaying the contents of a directory and selecting one of the mfiles
  3. Try pushing the loaded struct into a data table 
  4. Try pushing the data table into a struct
  5. Try going back down to an mfile again
  6. Do steps 1-5 with a user-definabel function
  7. Then start doing load_model(preproc, downsamp, modelfn) to initialize the GUI, using the above
  8. mod = load_model(preproc, model, stochasticity); % Load the model form
  9. Load the model parameters (save under cellid+stimfile in a 'saved' dir)

* BACKLOG
  - Add a button to load a set of model params and initialize the GUI accordingly
  - Normalize the PSTH
  - Handle NaN's better
  - Use resample during the downsampling step. (which applies the Sinc() function)
  - Modularize the Prefilter setup, which must have
    1. A default parameters object
    2. A function which filters a sound response
    3. A function which returns or plots the 'stationary frequency response'
    4. A pack function, which accepts 
    5. An unpack function
  - Automatically try to load Bandpass information from the arg_params struct passed to the bandpass filter fn (Look at exptparams.TrialObject.ReferenceHandle object?)

* Modules
  - Automatic covering bands filter, which presents downsampled results of gamma or elliptical notched filters spaced across the domains and completely covering the input space
  - Make a second prefilter (full-coverage gammatone bank)
  - Make a 3rd prefilter (full-coverage elliptic filter bank)
  - Increase the size of the GUI viewports  and clean up the labeling

* Koans to reflect upon
  1. Is it faster to prefilter with many different settings, then fit or correlate to each of them, rather than include it in the optimization loop?

* Notes on Stephen's Brain + Code Dump
  cellxcmaster('por012c-b1',238); % intelligently performs batch analysis 238 on cellid 'por012c-b1'
  After the execution of the above, 'params' contains the details of how the analysis was performed.
  params.resploadparms{1} is a way of getting
  params.respfiles gives a list of the files being used during the analysis
  dbget('sBatch', 238); % Returns details about which experiment is actually being performed
  
* There are three very important functions to look at:
  [cellfiledata, times, ...] = cellfiletimes()      % Note that times contains important info about the training set/test set split, such as the fitting method used?
  xcloadfiles      % Performs analysis on multiple files, queries from the database
  xcloadstimresp   % A cleaner, gentler version of the previous file that is probably what I should base my analysis off of. 
 
* Analysis on paper
  Where should the line be drawn between analysis in the DB, partitionining things for your search within the DB, holding out data, etc?
  How should the code accomodate extra dimensions of training in the future, such as behavioral ones? (Even though we don't think we need it now)
  CellID Dimensions: RespFile(), Stimulus #, repetition #, Value at Time
  Other dimensions: PreFilter #, Central filter #, 

* Issues for Stephen :
  1. Where is 'repetitions' visible? The closest thing I see is the 'Ref_Subsets' field returned in the 'parms' struct by 'dbReadData'

* Possible refactoring
  1. Data ordering is perhaps nonstandard, since we need filter(B,A,X,[],2) instead of filter(B,A,X);
  2. Should PREFILTEREDSTIM be a 3D matrix, or is it more convenient to use as a mixture of cell array and 2H matrices.? 
     STIM [30x400000] (30 tones with 400000 samples in time each)
     RESP [30x400000x3] (3 reps)
     PREFILTEREDSTIM{numoffilters} and under each cell [30x400000]
  3.  Rewrite of dbchooserawfile() because it's so damn useful for selecting a file, but let's make it work for multiple stimulus files
      (Should also display well, site and have selectors for channel, unit, etc
  4. Use squeeze() to remove unneeded dimensions from a matrix.
  5. Try filtfilt to avoid affecting the phase of the response
  6. Why is it 'stimpath' and 'stimfile' but 'path' and 'respfile'. it should be 'resppath'?
  7. Why is loadspikeraster the only thing that cares about the 'options' struct?

* CODE TO REVIEW LATER
  - [X] meska_pca()                              Used for doing the spike sorting, the front end. 
  - [ ] RemoteAnalysis/boost_online.m
  - [ ] Utilities/cacheevpspikes.m
  - [ ] cellDB/dbchooserawfile.m
  - [ ] Config/lbhb/BaphyMainGuiItems.m  has some hard-coded defaults for the GUI
  - [ ] Try messing with creating GUIs for structs using 'structdlg.m'
  - [ ] Consider the data for this: /auto/data/daq/Portabello/por010/por010c08_p_SPN
* LUXURY TODO
  - [ ] Write a function which MAKES A BACKGROUND COPY of the GS data so that you can 'hold' a model as a reference and play around with other settings, and see the results graphically.
  - [ ] Write dbchoosecellfiles()
  - [ ] Phase align in gammatone filter bank
  - [ ] make raw/stimulus response have two dropdowns to pick out colorbar thresholds for easier visualization
  - [ ] Make DB loading have adjustable rasterization freqs
  - [ ] Use inter_curve_v3 to interactively make FIR things!
  - [ ] Make the stimulus data drive the windowing of the other visualizations
  - [ ] Try adding color to histograms and scatter plots
  - [ ] try improving contrast of various intensity plots
  - [ ] Add BIC or AIC to model comparison data
  - [ ] Optimization report card and status information logged
* Possible problems or hacks to study
  - [ ] Negative effects of discretization on Inter-Spike Intervals histogram estimation (Use known data)
  - [ ] Infer the average rate of spiking from the data, then fit your model against that inferred lambd without doing EM all the time.
* HIGH LEVEL TODO:
   1) [ ] Logging and recording multiple models and their performance
   2) [ ] Plots the STRF of the best-fitting model?
   4) [ ] Analyze:  'dai020a-c2', 'mag009b-b1', 'dai008a-c1', 'mag007d-d1' 
   5) [ ] Rank model fits and plot correlations
   6) [ ] Replicate Stephen's results with exitation/inhibition
* ModelFit GUI Design Brainstorm
