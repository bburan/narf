* ROADMAP
  1. [ ] Start using queue system for starting model training files
  2. [ ] Across-file NPNL 
  3. [ ] Cleaning, bugfixes, and user interface changes that have accumulated
  4. [ ] Address question: Does variation in neural fuction in A1 follow a continuum, or are there visible clusters?

* WORKFLOW CHANGES
  Some unmet needs are becoming clearer:
  - [X] Need for a GUI summary browser (by cellid), with a link to launch the model stack, and a filter to pick only certain models (for viewing/comparison)
  - [X] Need to cache performance, git hash, modelfile, etc in mysql database or in a local ACID type database
  - [X] Need to generate reports sorting each model by some criteria 
  - [X] Need for comparing two models visually at the same time. 
  - [X] Need for different model combos to be tried on different batches (don't tie them together)
  - [X] Need to try things that aren't pure flat model combinations (Depression, but NOT with other things)
  - [X] Need to cache PNG images from each model's stack rather than rebuild every time
  - [ ] Need a way of marking a particular 'block' of modules so that particular 'plots' can be extracted despite positional differences which may occur
  - [ ] Need to compare between models trained on different subsets (SPN vs TSP), and support for BAPHY to express this concisely.
  - [ ] Have a display of the Pareto front (Dominating models with better r^2 or whatever)
  These needs could be met by:
  1. Using Baphy (OR flat file directories with tiny MAT files) to cache:
     - information
     - model file link
     - png file links
  2. Writing a cellid report browser which:
     - Has filtering fields: cellid, training_set1, training_set2, training_set3
     - Displays a scatter plot of the r^2 test/train performance of each displayed model
     - Lists matches from baphy, sorted by r^2, with (all?) stack plot PNGs, and a button to launch the stack viewer
  3. Updated stack viewer
     - Displays name, training set, test set, and other important info at top of window
     - Standardized colors for plots
  4. Putting an abstraction between Baphy for cellid selection and create a simpler batch structure
     - Batches now are just lists of cellids paired with training sets and test sets, and a model-perturbation tree of keyworded functions.
     - I'm going to disallow 'fitting' routines which mix and match data sets. If you want this, do it by hand!
  5. Replacing 'model groups' abstraction with a list of arbitrary nested perturbing functions (with associated keywords)

* WORK
** NEXT
   1. [ ] REFACTOR STUFF:
      - [ ] Write a pushing function which pushes a model into the database (not overwriting if it already exists)
      - [ ] Push existing files into the database
      - [ ] change fit_models() code to do a database push 
      - [ ] Fix the damn bug with NaNs in sparsebayes fitter!?
   2. [ ] Partition code into fitting and plotting sections
   3. [ ] Fix EM conditioning error and get gmm4 started again (Not sure how to fix!)

** MINOR ADDITIONS
  1. [ ] If empty test set is given for a cellid, what should we do? Hold 1 out cross validation? 
  2. [ ] A 2D sparse bayes approach. Make a 2D matrix with constant shape (elliptical, based on local deviation of N nearest points) to make representative gaussians, then flatten to 1D to make basis vectors fed through SB.
  3. [ ] Alternatively, try doing 1D sparse bayesian trick once on each dimension separately, and then combining the two dimensions together. 
  4. [ ] Add a keyword with a log compressor as an extra channel
  5. [ ] More concise summary of a cellid's models. HTML data table output?
  6. [ ] Plot both test and training data for smooth scatter plots.
  7. [ ] Standardize colors for plot lines. 
  8. [ ] A histogram heat map of model performance for each cell so you can see distribution of model performance
  9. [ ] A way to compare nonlinearities for multiple data sets.  
  10. [ ] Can I hide windows when plotting on them so they don't pop up and ruin my day when coding?
  11. [ ] FN: 'set_module_field' (finds module, sets field, so you can mess with things later in scripts)

** CLEANING AND BUGFIXES:
  1. [ ] Ooops, the git commit is not really being saved! It should go in META, not just in the cache
  2. [ ] CLEAN: Compare_models needs to sort based on training score if test_score doesn't exist.
  3. [ ] CLEAN: sf=sf{1}; should be eliminated IN EVERY SINGLE FILE!
  4. [ ] CLEAN: 'unused' should become ~ IN EVERY FILE
  5. [ ] CLEAN: Fix fit_sparsebayes() so that it won't ever have positive definite problems. eg, positivedefinite = all(eig(basis) > 0) OR use the 2nd return value of chol() when running the decomposition in sparse bayes
  6. [ ] REFACTOR: Replace all the 'true' and 'false' arguments with textual flags and varargin that are more descriptive
  7. [ ] REFACTOR: the Fitters because they are all pretty much the same damn thing over and over again
  8. [ ] Replace my nargin checks with "if ~exist('BLAH','var'),"
  9. [ ] Grep for TODO's, FIXME's, etc in existing files
  10. [ ] Search for obviously dead code and bury it in the graveyard
  11. [ ] Search for repeated blocks of code and refactor
  12. [ ] Put proper docstrings on every function in util/
  13. [ ] Check paths again, grep for NARF_PATH and correct (also: replace / with filesep when possible)
  14. [ ] Names probably could use some rethinking as well, especially defaults (like using 'stim' default even in the fitting algorithms, for example)
  15. [ ] Add error handling (catch/throw) around EVERY CALL to a user defined function
  16. [ ] In retrospect, 'gui' and 'plot_gui' stuff probably shouldn't be stored in the XXX or STACK structures...move it be in a 3rd structure?
  17. [ ] Ensure that no closures of data are being done by methods. Methods should accept the module object as their first argument, not close over anything.
  18. [ ] It's not quite right to have the 'replot' command be part of the the 'plot_popup fn callback'. Needs to be re-thought.
  19. [ ] Create a module methods directory for shared methods
  20. [ ] Create a module keywords directory for helping with combinatoric name management instead of 'module groups'? 
  21. [ ] make anything named 'do_' into a method for use with modules?
  22. [ ] make anything named 'update_' into a function used purely for its side effects?
  23. [ ] Delete the GUI objects whenever you 'apply' since they may need to be recreated?
  24. [ ] Make sure that fitters return specific codes indicating how they terminated

** FITTERS:
  1. [ ] FITTER: ARD + ASD
  2. [ ] FITTER: A stronger shrinkage fitter (Shrink by as much as you want).
  3. [ ] FITTER: Log Likelihood because MSE is biased towards gaussian noise models, and for real-life data sets the probability tails are always heavier than a gaussian. 
  4. [ ] FITTER: Three-step fitter (First FIR, then NL, then both together).
  5. [ ] FITTER: Multi-step sparseness fitters (Fit, sparseify, fit, sparsify, etc). Waste of time?

** LARGER WORK
  1. [ ] Generalize N-step fitter to let a particular module or set of modules float. 
  2. [ ] inter_spike_intervals 
  3. [ ] bayesian_likelihood() perf metric
  4. [ ] Automatic Relevancy Determination (ARD)
  5. [ ] Automatic Smoothness Determination (ASD)
  6. [ ] Use a single wavelet transform in place of downsampling + FIR filter (Hard and slow to fit, but extremely general)
  7. [ ] Roll model summary caches and select_summaries into Stephen's BAPHY, since in the end all I did was reinvent yet another crappy RDBMS

** MODULES:
  1. [ ] MODULE FN: Provide functions to cover the input space logarithmically with filters
  2. [ ] MODULE: Make a faster IIR filter with asymmetric response properties 
  3. [ ] MODULE: Make Concat Second Order Terms work for any higher order nchoosek type stuff
  4. [ ] MODULE: Standardized single/multi channel gammatone filter
  5. [ ] MODULE: Standardized single/multi channel elliptic filter 
  6. [ ] MODULE INIT: Make a module which has a complex init process
	 1) Creates a spanning filterbank of gammatones
	 2) Trains the FIR filter on that spanning filterbank
	 3) Picks the top N (Usually 1, 2 or 3) filters based on their power
	 4) Crops all other filters
  7. [ ] MODULE FN: Provide an auto-init for the filters which cover the input space, train filters on that, and picks the channel with the most power. It does this once wide, then once narrow.
  8. [ ] MODULE: Add a module which can pick out a particular dimension from a vector and give it a name as a signal
  9. [ ] MODULE: Build a non-cheating model which extracts envelopes directly from the WAV files using an elliptic or gammatone prefilter

** END USER CONVENIENCES
  1. [ ] Why isn't auto recalc the default?
  2. [ ] Make logging work for the GUI by including the log space in narf_modelpane?
  3. [ ] IRRITATION: Why doesn't 'nonlinearity' module default to a sigmoid with reasonable parameters?
  4. [ ] IRRITATION: Why doesn't it show the model save filename so I can see which file I just loaded if I forgot?
  5. [ ] IRRITATION: Why doesn't every plotted signal have a legend?
  6. [ ] IRRITATION: Why don't the X (or at least the Y) axes have scales?
  7. [ ] IRRITATION: Why isn't there an 'undo' function?
  8. [ ] IRRITATION: Why can't I resize windows?
  9. [ ] IRRITATION: Why isn't there progress in the GUI when fitting?
  10. [ ] IRRITATION: Why are the editable text boxes so damn small?
  11. [ ] IRRITATION: Why can't I edit a module type in the middle of the stack via the GUI?
  12. [ ] Write a crash course guide on using NARF
  13. [ ] Remember to invalidate data BELOW the present point on a table-edit callback... and to update the gui to reflect this!
  
* DESIGN QUESTIONS TO BRAINSTORM:
  1. [X] How can sane initial conditions for optimization be automatically arrived at without extra script-writing?
	 Auto-initialization of model params is done by allowing modules to update their design based on the data by calling the optional 'auto_init' method.
	 Arg 1 is the STACK, not including the model itself. 
	 Arg 2 is the XXX data input, not including the model's output data itself. 
  2. [X] How can jack-knifing be integrated in to the optimization routine to prevent over-fitting?
	 Split the big long RESP and STIM vectors in fit_with_lsqcurvefit into 10 chunks
	 Take groups of 9 of those chunks, run lsqcurvefit, then test on remaining chunk
	 Take weighted average of all jackknifed solutions, weighting each by inverse variance? Or just mean, if we assume they all have same variance?
	 Return weighted average.
  3. [X] How should optimization constraints be incorporated in the design?
	 Probably the easiest way is to define a structure which may be used by pack/unpack to create upper and lower bounds, which are then passed to the optimization routine
	 opt_hints = struct('alpha', [-1 3], 'beta', [0 inf]); % Constrain alpha from -1 to 3 and beta from 0 to infinity. 
  4. [X] How should models be automatically generated in a quick and scriptable way?
	 See analysis/test_likely_candidates.m
  5. [X] How can design internal degrees of freedom be detected and corrected during optimization?
	 (Probably they cannot!)
  6. [X] There needs to be a place to store information about a whole model. 
	 For example, 'model name' and 'fitter' are two examples of fields that don't really belong in a module.
  7. [X] There is no best fitting routine, only fitting routines which work better for different cells. Allow them all a chance to run by making them module parameters.
  8. [X] Can jackknifing or the equivalent be applied to ANY fitting routine as a higher level function
	 If we only have one data file, how can we hold out some fraction of the stimuli so that we can do training/test on a single data file?
	 Solution:
	 - Fit routines use a 'score'
	 - The stack gives the score
	 - The score needs to be calculated from a jackknife
	 - How can data be jackknifed without modifying the stack?
	 - Immediately after the loading, zero a chunk of the stim and respavg (save the original, of course)
	 - Do a fit with whatever routine you want
  9. [X] N-step fitter (train FIR in common, train NL across each separately)
	 Surprisingly difficult to make several models need to be fit all on the same data. yet ALSO need to run on different behavioral states. 
         1. Violates my implicit expectation of 1 fitter -> 1 model. Now I have 1 fitter-> many models.
	 2. Now that training_set{} may be edited, it shouldn't really be copied from one XXX{1} to XXX{2} and so on.
	 Solution ideas: 
	 - Quick hack: five new fitters added
	   NL1, trains on all, but only trains NL on 1st
	   NL2, trains FIR on all, but only trains NL on 
  10. [ ] Right now, it's very convenient to be able to have the 'fitter' and 'score' quantity to be in modules
	  I can plug in all the module groups and let the fitter run. I can compare different fit routines automatically.
	  However, a fitter is not really part of a module, it's part of a whole model.
	  Therefore, in the future, the fitter and score quantity should be stored in the model META structure.
	  On the other hand, I need to justify this: Why should this be done instead of leaving it in the STACK? What we have right now works and is convenient.
	  (Because we may want to try multiple fit routines, and pick the model with the best training score?)
	  (Because I expect that model specific fitters are necessary? That isn't a reason!)
	  ANSWER: A better way to achieve this type of thing would be to have mutating functions which mutate a default copy of the stack. (Kind of like how calculus of variations work)
	  By picking and choosing and intercombining these mutating functions, you could come up with many different variations.
	  They also would not be restricted to the somewhat arbitrary groupings which I came up with, and would let multiple parts of stack be mutated simultaneously.
  11. [ ] Right now, you can only instantiate a single GUI at a time. Could this be avoided and the design made more general?	  
	  To do this, instead of a _global_ STACK and XXX, they would be closed-over by the GUI object.
	  Then, there would need to be a 'update-gui' function which can use those closed over variables.
	  That fn could be called whenever you want to programmatically update it. 	  	  	 
  12. [ ] It is awkward in non-parametric non-linearity module to recalc the phi every time you need it for graphing. Some place to cache it would be good without risking cache staleness.
  13. [X] Nonparametric Nonlinearity (NPNL) linearizes anything. 
	  It is very much data-driven, which is great. 
	  On the other hand, it fits itself to linearize almost anything, so we somehow learn less than a simple, parameter-driven model. 
	  How can we balance complexity in the FIR or complexity in the NL?
	  ANSWER: Sparseness needs to be modeled on the FIR side, Smoothness on the NL side. 
  14. [X] How can LSQ curve fit use sparseness and smoothness metrics?
	  You can cheat and destroy the module system by looking later in the STACK for the MSE element. 
	  If the MSE module exists and has nonzero weights, add a bogus zero element the LSQ target vector, and a bogus LSQ prediction vector element with a value of the sqrt(smoothness_penalty).	  
  15. [ ] Are neurons clusterable according to which models describe them well?
	  Are they really different populations of neurons, or just points along a continuum?
  16. [ ] Even auto-generated models should have their own 'summarize' function which plots relevant data?
  17. [ ] ENDGAME: 
	  Is the end goal of this system something that:
	  - Spans the input space of nonlinearities?
	  - Spans the input space of depression?
	  - Has an inhibition and excitation filter?
	  - Has a NPNL for inhibition, and a NPNL for excitation?
	  - Uses ARD to eliminate all unimportant dimensions?
	  - Reports the best model?

*  UNESSENTIAL TODO ITEMS
  - [ ] Make gui plot functions response have two dropdowns to pick out colorbar thresholds for easier visualization?
  - [ ] Make it so baphy can be run _twice_, so that raw_stim_fs can be two different values (load envelope and wav data simultaneously)
  - [ ] MODULE: Add a filter that processess phase information from a stimulus, not just the magnitude
  - [ ] Write a function which swaps out the STACK into the BACKGROUND so you can 'hold' a model as a reference and play around with other settings, and see the results graphically by switching back and forth.
  - [ ] Try adding informative color to histograms and scatter plots
  - [ ] Try improving contrast of various intensity plots
  - [ ] Put a Button on the performance metric that launches an external figure if more plot space is needed.
  - [ ] Add a GUI button to load_stim_from_baphy to play the stimulus as a sound
  - [ ] FITTER: Crop N% out fitter:
	  1) quickfits FIR
	  2) then quickfits NL, 
	  3) measures distance from NL line, marks the N worst points
	  4) Looks them up by original indexes (before the sort and row averaging)
	  5) Inverts nonlinearity numerically to find input
	  6) Deconvolves FIR to find the spike that was bad
	  7) Deletes that bad spike from the data
	  8) Starts again with a shrinkage fitter that fits both together
  - [ ] Expressing NL smoothness regularizer as a matrix
	  A Tikhonov matrix for regression: 
	  diagonals are variance of each coef.
	  2nd diagonals would add some correlation from one FIR coef to the next (smoothness?).
  - [ ] Sparsity check:
	 For each model,
            for 1:num coefs
             Prune the least important coef
              plot performance
            Make a plot of the #coefs vs performance
  - [ ] A check of NL homoskedasticity (How much is the variance changing along the abscissa)	     
  - [ ] FITTER: SWARM. Hybrid fit routine which takes the top N% of models, scales all FIR powers to be the same, then shrinks them.
  - [ ] Get a histogram of the error of the NL. (Is it Gaussian or something else?)
