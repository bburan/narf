* Next steps:
  - How can I define functions such that the input of one chains to the other, using a proper data structure
  - Instead of loading in response at 100Khz, load it in at 10Khz. (The first function should be the load_from_baphy_rasterization functions)
  - Write a generic pack/unpack function to expose parameters to the optimization
  - Make a function that accepts a cell array of parameter structs and a parameter vector, and returns a new cell array of parameter structs
  - Add a gammatone bank preprocessing stage
  - Add a button to "save analysis", connecting the stimfile train/test sets, model structure and params, optimization method, and GIT code hash number all together in a single, savable structure.
  - Add a button or textbox to "load analysis" settings that have already been found, so that saved data from a batch can be quickly browsed.
  - Allow plot methods of objects to use a popup menu and label. 
  - Replace all the 'length' and 'size' things with 'numel' where appropriate
  - Normalize the binning as a rate in the very first step
  - Add error handling (catch/throw) around calls to user defined functions
  - Handle NaN's better...right now they cause problems!
  - Use resample during the downsampling step. (which applies the Sinc() function)
  - Automatically try to load Bandpass information from the arg_params struct passed to the bandpass filter fn (Look at exptparams.TrialObject.ReferenceHandle object?)
  - Add a filter that processess phase information from a stimulus
  - make raw/stimulus response have two dropdowns to pick out colorbar thresholds for easier visualization
  - Make DB loading have adjustable rasterization freqs
  - Go through the TODO's in my files and look for more

* Stage Three (For a Later Version)
  The next step up the abstraction tree, if we need to, is to replace the fixed number of function calls with an arbitrary number.
  I envision the modeling process being a chain of function calls, each of which accept a cell array of parameter structs P and a N-dimensional matrix X, and return a new P and new X.
  Outputs of one function are chained to the inputs of the next. 
  If any intermediate parameter struct is modified, then it invalidates all cells after it and the computation must recommence from that point. (I think continuation-passing-style works well here)
     If you need to do different 'branches' of computation, you can store the pointer to the current computation point and save them.
  I envision each function being loaded from a directory and having its parameters GUI editable in the way that they are now. 
  I envision that each function has an associated graph window with a dropdown, and is populated by asking that function object which plotting methods it has.
  When we put code in the DB, we should ALSO carefully record the GIT hash which the analysis was run on. Subtle differences or changes my occur over time.

* Allowed Dimensions: How should can we accomodate the later addition of extra dimensions in the future, such as behavioral characteristics?
  Right now we have:
  1. StimFile               (Which is not indexed, but uses a keyword)
  2. Stimulus # 
  3. Value at time
  4. Repetition #
  5. Preprocessor Index #   (Because preprocessing may have multiple dimensions)
  In the future, we may have more. 
  The only way I can think about allowing multiple dimensions to vary arbitrarily would be to either:
  A) Somehow keep track of their numerical indexes as you go along, using a struct
  B) Avoid numerical indexes and use struct arrays or cell arrays everywhere? 
  Overall, option A sounds like the more efficient choice

* Tricky things:
  We may need to do an iteration procedure that treats one part of the model (IE, Linear FIR filters) differently from a nonlinear part (In my opinion, this is just a special case sampler)
  If you modify a function after starting up narf_gui, what will happen? (Right now, changes to the pretty-name and params will not be altered without restarting narf_gui, however if you fix the function itself then that is fine.)

* Issues for Stephen :
  1. Where is 'repetitions' visible? The closest thing I see is the 'Ref_Subsets' field returned in the 'parms' struct by 'dbReadData'

* Possible refactoring
  1. Data ordering is perhaps nonstandard, since we need filter(B,A,X,[],2) instead of filter(B,A,X);
  2. Should PREFILTEREDSTIM be a 3D matrix, or is it more convenient to use as a mixture of cell array and 2H matrices.? 
     STIM [30x400000] (30 tones with 400000 samples in time each)
     RESP [30x400000x3] (3 reps)
     PREFILTEREDSTIM{numoffilters} and under each cell [30x400000]
  3.  Rewrite of dbchooserawfile() because it's so damn useful for selecting a file, but let's make it work for multiple stimulus files
      (Should also display well, site and have selectors for channel, unit, etc
  4. Use squeeze() to remove unneeded dimensions from a matrix.
  5. Why is it 'stimpath' and 'stimfile' but 'path' and 'respfile'. it should be 'resppath'?
  7. Why is loadspikeraster the only thing that cares about the 'options' struct?
  8. Where should the line be drawn between analysis in the DB, partitionining things for your search within the DB, holding out data, etc?

* CODE TO REVIEW
  - [X] cellxcmaster('por012c-b1',238); % intelligently performs batch analysis 238 on cellid 'por012c-b1'
  - [ ] After the execution of the above, 'params' contains the details of how the analysis was performed.
  - [ ] params.resploadparms{1} is a way of getting
  - [ ] params.respfiles gives a list of the files being used during the analysis
  - [ ] dbget('sBatch', 238); % Returns details about which experiment is actually being performed
  - [ ] [cellfiledata, times, ...] = cellfiletimes()      % Note that times contains important info about the training set/test set split, such as the fitting method used?
  - [ ] xcloadfiles      % Performs analysis on multiple files, queries from the database
  - [X] xcloadstimresp   % A cleaner, gentler version of the previous file that is probably what I should base my analysis off of. 
  - [X] meska_pca()                              Used for doing the spike sorting, the front end. 
  - [ ] RemoteAnalysis/boost_online.m
  - [ ] Utilities/cacheevpspikes.m
  - [X] cellDB/dbchooserawfile.m
  - [X] Config/lbhb/BaphyMainGuiItems.m  has some hard-coded defaults for the GUI

* LUXURY TODO
  - [ ] Write a function which swaps out the GS into the BACKGROUND so you can 'hold' a model as a reference and play around with other settings, and see the results graphically by switching back and forth.
  - [ ] Write dbchoosecellfiles()
  - [ ] Use inter_curve_v3 to interactively make FIR things
  - [ ] Try adding color to histograms and scatter plots
  - [ ] try improving contrast of various intensity plots
  - [ ] Add BIC or AIC to model comparison data
  - [ ] Optimization report card and status information logged
  - [ ] Take the STRF of a model, not of the data!
  - [ ] Analyze:  'dai020a-c2', 'mag009b-b1', 'dai008a-c1', 'mag007d-d1'
  - [ ] Rank model fits and plot correlations

* KOANS
  The fastest way to climb a tall mountain is to accept that you must occasionally descend when you find yourself on the wrong path.
